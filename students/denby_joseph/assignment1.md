Joseph Denby  
Dr. Benjamin Soltoff  
MACS30000  
Assignment 1  
October 16, 2017  

Within the past decade, [Airbnb](airbnb.com) has become an increasingly popular means for travelers to find lodging. Using Airbnb's website as a portal, local residents can open up their homes to guests, providing an alternative to the traditional hotel route. Users looking for a place to stay create a profile and submit interest to renters for a particular set of dates. Renters then vet from among the prospective lodgers, picking one who gets to stay.

While Airbnb is a great service for finding a cheap bed or making easy money off of an empty guest bedroom, its model has run into some issues of discrimination. In 2016, accounts of racially biased renting decisions surfaced, even resulting in a trending social media [hashtag](https://www.theguardian.com/technology/2016/may/05/airbnbwhileblack-hashtag-highlights-potential-racial-bias-rental-app). A corroborating [Harvard study](http://www.benedelman.org/publications/airbnb-guest-discrimination-2016-09-16.pdf) analyzing thousands of applications found that users were 16% less likely to be accepted as a lodger if they possessed a "distinctively African-American name." These results are both stunning and disheartening, and they merit further investigation. 

Because Airbnb markets itself as a user platform and not a business unto itself, it is (ostensibly) not subject to the same anti-discrimination laws that hotels and the like uphold. But, because renters can choose their guest, the platform offers a unique way to probe the prevalence of racial biases amongst its users. I propose using Airbnb's user data to assess racial attitudes in the United States by matching racially distinct users and comparing acceptance rates. 

In order to assess bias computationally, I would compare each user's number of successful stays to the total number of persons contacted. This design assumes that if a user contacts an individual but does not stay with them, the discrepancy is due to racial bias. This is a slight extrapolation, but it is not entirely unreasonable, given that there aren't many other reasons for a renter to deny a stay to an otherwise trustworthy user. A user cannot contact a renter to inquire about unavailable dates, so any denials are, in large part, due to the renter deciding not to rent to that particular user. This method may capture some noise, but it is a good proxy to get a reasonable estimate of racial bias.

My research design would involve using Airbnb's proprietary dataset, which is not currently public. However, Airbnb should be willing to share their information to this study for two reasons. First, they should be willing to make every effort possible towards identifying and rooting out users with racist biases on their platform, if only because tolerating racism reflects poorly on their brand and their business. Secondly, if certain users are having negative experiences based on their race they are less likely to use Airbnb; either they actively recognize biases in other users, or they simply aren't having as much of an opportunity to use the service due to fewer approved stays.  By eliminating discrimination on their platform, Airbnb would effectively increase their active user base, growing their company.

The above plan satisfies the framework for an observational study that 'measures' rather than simply 'counts' things. There is obviously no explicit value for racial bias in Airbnb's user metadata, so one has to be approximated using information that *is* available. By using the information described above, I can reliably estimate racial bias in user interactions. 

My plan to track discrimination among Airbnb users would involve matching similar users of different races, so I could attribute as much discrepancy in  acceptance rates to race as possible. Airbnb doesn't appear to collect much demographic information about its users, but I could match users based on self-reported gender, home location, and items in the "Describe Yourself" section. Because there is no self-reported race or ethnicity in a user's profile, I would have to solicit that directly from the users themselves. This would also provide me an opportunity to ask a user's permission for their information, avoiding any ethical quandaries. 

After collecting these data and assessing racial biases, I might be able to use my findings to forecast user experiences according to their race or prospective location. By measuring the rate at which users of different races are denied stays in different locations, I could predict user experience and take action to prevent racial prejudice (e.g., hiding a user's profile picture, anonymizing interactions between users and renters, etc.).

By employing a large dataset, this research design is prone to certain benefits and pitfalls. First, the sheer size of Airbnb's platform is a huge asset. With [estimates](http://fortune.com/2017/03/07/airbnb-ceo-hosts/) of 150 million users and 3 million hosts, matching Airbnb users and trawling records of stays and messages should be easy and will yield statistically robust results. Because Airbnb's user base is so large and spans the entire country, it would be possible to run this analysis on subpopulations according to race, gender, ethnicity, location, age etc., producing results that Airbnb can use to address specific issues as they arise. 

Next, Airbnb's website is always-on, meaning users and hosts are constantly interacting, so this study can analyze trends in racial discrimination over time. By using this measure, Airbnb can identify how racial biases might have changed over the course of its existence, potentially linking trends in bias to notable events at a local, national, or international level. Further, Airbnb could use this measure to test different strategies for halting racial discrimination in its user base – by A/B testing different strategies in different locations and comparing results, Airbnb can determine the best solution for the problem at hand. 

This research design takes advantage of Airbnb's user base being non-reactive as well. Because this analysis would track many users over a long period of time using relatively non-sensitive information, Airbnb's users are unlikely to alter their behavior in response to this study. By using this data set to probe racial biases, this design avoids the [Hawthorne effect](https://www.wikiwand.com/en/Hawthorne_effect), whereby participants in a study tend to alter their behavior (sometimes unconsciously) due to their conscious awareness of study participation. The Hawthorne effect is unlikely to occur here, so the measure will be a better judge of racially biased attitudes. 

This research design is also prone to some of the drawbacks of using big data, but thankfully they can be largely avoided. To start, this data set is publicly inaccessible, as previously mentioned, but Airbnb would likely be amenable to sharing their data in order to combat racism on their platform. 

As with all online platforms, Airbnb's user base potentially faces the issue of non-representation. If this study aims to use data on Airbnb's users in order to probe racial biases in America, it assumes that Airbnb's user base is a representative sample of the United States population. According to Airbnb's publicly available demographic data, this is largely true. Airbnb's user base consists of [rougly equal proportions of men and women](https://techcrunch.com/2015/09/07/airbnb-hosted-nearly-17-million-guests-this-summer/?ncid=rss), and is only slightly skewed towards a younger demographic, with an average user age of 35. So while its user base is not a perfect sample of the population of the entire United States, it is workably close.

In sum, this observational research proposal to use Airbnb user data to assess levels of racial discrimination on the platform as well as in the United States as a whole, takes full advantage of the range of opportunities that big data analysis has to offer, while dodging several of the accompanying pitfalls as well. 
    