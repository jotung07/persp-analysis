Joseph Denby  
Dr. Benjamin Soltoff  
MACS30000  
Assignment 3  
October 30, 2017  

As stated in my previous research proposals, I am interested in assessing levels of racial discrimination on the popular rental sharing platform [Airbnb][airbnb]. Airbnb allows users to rent their properties to others individually; typically some reasonable vetting occurs, ensuring that the prospective renter is reputable and trustworthy. Recently however, some users [report particular trouble][hashtag] getting approved for stays, suggesting racial discrimination as the culprit. Not long after, a research team [investigated][Harvard] this claim experimentally. This publication concluded that Airbnb users with distinctively African-American names were 16% less likely to receive a positive response to a stay inquiry when compared to equivalent users with distinctively White names. This experiment's findings are interesting and point to a fruitful area of further research. In order to more thorougly investigate the prevalence of racial discrimination on Airbnb, I propose an experimental research design involving the creation and deployment of fake accounts with racially distinctive names; by comparing relative positive response rates between users, one can more robustly evaluate the discrimination landscape on the platform. Specifically, I wish to employ an experimental research design to assess the effect of race on interactions with potential renters. 

This experiment would closely mirror that conducted in Edelman et al. (2016), with some important differences. This design would involve the creation of fake Airbnb accounts with names signifying a particular race (viz., White, Black Chinese, Indian, or Hispanic) and gender. These accounts would then send identical stay requests to renters, as a typical user on Airbnb would. By categorizing the responses as generally positive or negative and comparing the proportional distributions between each kinds of account, one can assess the general levels of discrimination experienced by users of a particular race and/or gender. With a large enough random sample of renters, the outcomes would track discrimination towards specific minority populations across the *entire* platform. 

This experiment will be __computationally-enhanced__ at practically all stages, seeing as it will take place through an online platform with its users as participants. The 'participants' will be renters on Airbnb who rent regularly. They will not knowingly take part in the experiment, as doing so would likely ruin the purpose of the research design (i.e., renters will not readily admit to discrimination). Instead, renters will randomly receive messages from the set of generated accounts and their responses will be categorized. Individual renters will be assigned to one and only treatment condition (i.e., they will receive a message from only one sham accounts) to prevent any change in behavior that would occur from receiving multiple messages from eerily similar empty accounts. Similar to the procedure in Edelman et al. (2016), renters' responses will be categorized according to a general positive/negative framework, filtering out extraneous responses (e.g., "I'll get back to you") and non-responses. More specifically, this experiment's analyses would compare the relative proportion of unequivocal "Yes" responses across treatment conditions. Computing and comparing the proportion of positive to negative responses among users of different races and genders will serve as a robust computational investigation of discrimination on Airbnb. 

To control for any influencing factors beyond race and gender, the fake accounts will be devoid of any information beyond a made-up name. To ensure that the names associated with the sham accounts are actually evocative of a specific race and gender, workers on Amazon Mechanical Turk will take part in small experiments in which participants will respond quickly with the race they associate with a given name. This type of task, employed  successfully in Edelman et al. (2016) and [Milkman et al. (2015)][Milkman], will verify that the names selected are adequate proxies for race and gender.

A potential issue addressed in Edelman et al. (2016) concerns the feasibility of this experiment given Airbnb's policies regarding fake accounts. According to the aforementioned publication, the experiment had to be concluded somewhat prematurely due to unexpected termination of many fake accounts by Airbnb itself. The proposed study cannot run this risk, seeing as it involves creating and employing many more fake accounts in order to represent various minority populations. In order to successfully run this study without sacrificing breadth or depth (and thus statistical power), Airbnb would be briefed and their consent obtained. Airbnb would likely accommodate this research design, seeing as its findings bear directly upon the success of its business. Revealing and addressing discrimination on the Airbnb platform stands to boost the business's success by ensuring a positive experience for all users; the potential benefit associated with this experiment outweighs any drawbacks due to, e.g., a temporary increase in 'fake' accounts on the platform. 

As presented, this experiment has the potential to assess heterogeneity of treatment effects; more specifically, it can determine the levels of discrimination faced by men and women of different races, with further levels of granularity possible with more specific treatment groups. At the outset,  renters would be selected randomly from the entire Airbnb renter population and assigned to a treatment group at random. So, any outcome measure of discrimination would pertain to the user base as a whole. With this particular design logic, heterogeneity of treatment effects would be difficult to assess, as the population from which renters are sampled is large and diverse, so all idiosyncratic treatment effects would be averaged together. However, conducting the experiment with this method of sampling would establish a robust baseline against which to test further hypotheses concerning heterogeneity of treatment effects. For instance, later studies could mimic the categorization of renters done in Edelman et al. (2016); by selecting renters from particular metropolitan areas or of a particular race and/or gender, this experiment could assess whether bias is also a function of local demographics or self-identification. 

Regarding validity, this experiment has relatively strong internal validity, potentially at the cost of external validity. Internal validity is to be understood as the capacity for an experiment to reliably and faithfully manipulate and measure the variable of interest. The fake accounts differ only in name, and, using Mechanical Turk, the names are assured as strong indicators of race and gender, so any substantial differences in renter response, averaged across relatively similar renters, would be solely attributable to race. In order to control for any potential confounds, the sham accounts would have to completely void of information beyond a name (e.g., photos, personal information, etc.), a fact which muddies internal validity a bit by making the fake accounts appear less believable. However, even if the emptiness of the fake accounts shift renters' responses to be more negative, any *relative* differences between accounts of different race or gender can only be attributed to the experimental condition, namely the user being of a particular race or gender. So, this design serves as a strikingly good way to probe discrimination by adequately controlling for any potential confounds.

On the other hand, the external validity of this experiment is more questionable. As the experiment takes place solely on the Airbnb platform using artificial accounts, it would be difficult to extrapolate any findings to a larger population. First, it is unclear how behavior between users on Airbnb might map onto real face-to-face interactions, or even online interactions on another platform. So, it would be challenging to legitimately make inferences about the prevalence of discriminatory attitudes on whole based solely on this experiment. Second, because the experiment's various treatment conditions involve a renters' response to a mostly void account, it is not clear how any findings concerning discrimination towards specific accounts would predict or explain a given renters' interactions with *real* users. Even if the experiment found that, on average, a certain minority population's accounts experienced greater levels of discrimination in their interactions, actual users belonging to that group may have vastly different experiences due to other factors that mitigate or exacerbate a baseline level of discrimination. This experiment's artificiality is integral to its valid investigation of the variable of interest, but it might come at a cost to its generalizability. 

On a related note, the experiment as stated is adequately controlled so as to make believable conclusions about causal mechanisms. As previously mentioned, an individual given user's experience on the platform, specifically their proportion of positive interactions with renters, is likely impacted by a variety of factors, including their name, profile photo, previous stays, reviews, etc. This study concerns itself with the impact of name-based racial and gender discrimination only, so it employs fake accounts containing *solely* that information. So, while this experiment cannot claim to fully explicate the factors that influence a given user's experience on Airbnb, it does serve as a robust investigation of one causal mechanism in particular, namely discrimination based on race and gender.

This experiment-based research design, as a means of investigating the general question of discrimination on Airbnb, has some specific benefits and drawbacks when compared to other research designs, such as observational studies or surveys. Most importantly, since this experiment design involves a particular manipulation accompanied by extensive control measures, it can most strongly claim its findings as revealing of underlying causal mechanisms. Surveys and observational studies rely primarily on self-report or mining of pre-existing data, so their findings are better categorized as simply descriptive of a particular (set of) population(s). However, since experiments involve careful construction of valid experimental conditions, it has the potential to require a large investment of time and/or money. By employing computational methods at various stages though (e.g., automation of account messaging, selection of participants, sorting into experimental condition, categorization of response, etc.), this particular experiment can ensure that its costs do not scale with the sample population. So, an experiment stands as an exceptionally efficient means of yielding robust conclusions about the research question. 

In sum, this experimental research proposal uses fake accounts with names indicative of a particular race and gender combination to assess average levels of discrimination on Airbnb. By categorizing and averaging random renters' responses to inquiries from these accounts, this experiment can claim to robustly assess the sole impact of race and gender on a user's experience on the platform. 


[airbnb]: https://www.airbnb.com
[hashtag]: https://www.theguardian.com/technology/2016/may/05/airbnbwhileblack-hashtag-highlights-potential-racial-bias-rental-app
[Harvard]: http://www.benedelman.org/publications/airbnb-guest-discrimination-2016-09-16.pdf
[Milkman]: http://web.a.ebscohost.com.proxy.uchicago.edu/ehost/detail/detail?vid=0&sid=e7d346d1-3bd0-4737-95c9-3a2ec678af75%40sessionmgr4010&bdata=JnNpdGU9ZWhvc3QtbGl2ZSZzY29wZT1zaXRl#AN=2015-15680-001&db=pdh
