Joseph Denby  
Dr. Benjamin Soltoff  
MACS30000  
Assignment 5  
November 13, 2017  

### Kaggle Competition
---

On Kaggle's [Competitions][comp] page, there is a currently ongoing competition called [Spooky Author Identification][kagglecomp]. The purpose of this competition is to develop and employ machine learning algorithms to identify the authors of horror stories using natural language processing. Specifically, the data include excerpts from stories written by Edgar Allen Poe, Mary Shelley, and HP Lovecraft, all of whom are horror authors whose work has entered the public domain. The data include two sets (for training and testing), each consisting of sentences and their respective author. The goal of the competition is to use the training data to tune some learning algorithm to correctly identify the author of each sentence in the testing data. In order to make a submission, one must upload a .csv file with one's algorithm's predictions for the test dataset. Included with the files for the competition is a sample submission; the file must include a row for every sentence in the test set, with the fields being the predicted probability of authorship for each candidate.   



### Journal Improvement  
---
In a recent issue of the [Journal of Psychological Science][JPS], [Lovett and Franconi (2017)][paper] investigate the means by which individuals compare images and identify differences. Specifically, they are interested in how individuals can quickly perceive changes in topological relations between objects. The researchers hypothesize that *categorical* changes (e.g., inside, below, above, beside) are more easily tracked than strictly metric changes; for example, when an object is situated next to another in one image and inside it in the next, that change is more noticeable than one where an object moves an equal distance but does not cross any categorical boundaries. To test this hypothesis, the researchers recruited participants (Northwestern University undergraduates), had them perform simple visual same/different comparison tasks, and analyzed collective accuracy according to the type of change that occurred (categorical or purely metric). According to the resulting data, participants on whole were significantly more accurate at detecting categorical changes than noncategorical changes of comparable magnitude, suggesting that cognitive topological relations include substantial categorical coding.

The experiments employed in the paper are clever, but would greatly benefit from the use of human computation. First and foremost, the sample size for this study is relatively small and circumscribed (62 undergraduates across four experimental conditions). Since the tasks are incredibly simple to perform, requiring little to no training, the researchers could have employed Amazon Mechanical Turk workers to collect substantially more data, making their findings more robust. Moreover, workers on Mechanical Turk would provide a more representative sample of the population of interest (all humans) than undergraduates from an elite university, reducing representation error.  Secondly, and perhaps most interestingly, taking advantage of the vast human computation power available through Mechanical Turk would allow for a more robust testing of the hypothesis in general. As it stands, this research design uses exclusively simple circles as objects in its various experiments. While the choice of this basic geometric shape controls for some noise, its artificiality makes it unclear how the experiments' resulting data extend to conclusions about topological coding more generally. Since Mechanical Turk offers a massive subject pool at marginal cost, researchers could afford to break up this large and complicated question into many simple experiments (e.g., different shapes, colors, objects, distractors, etc.), collect mass amounts of data, and combine results in various ways to yield more rigorous and robust insights into the phenomenon in question. Mechanical Turk would allow the researchers to run many experimental conditions in parallel and en masse, meaning they could yield significantly more powerful and statistically supportable results in a time comparable to that spent with the original research design.  

### InfluenzaNet
---
1. Through [InfluenzaNet][IN], researchers aim to track the prevalence, spread, and severity of influenza through self-reported data produced by individuals with influenza-like illnesses (ILI). This approach employs human computation (through web-based surveys) to yield relatively accurate and up-to-date information about current influenza trends at a reasonable cost. [Google Flu Trends][GFT] does something similar, but instead bases its analyses on data from internet searches pertaining to the flu. Because this method mines existing data instead of sourcing it from self-report surveys like InfluenzaNet, it avoids some representation error caused by self-report bias. Google Flu Trends is also very cost-effective, as it employs solely computation as its means of data collection and analysis and is thus massively scalable. Traditional methods of influenza tracking involve direct correspondence with physicians and/or analysis of past and current viruses to determine the trajectory and severity of the current influenza strain. These analyses are not as cost effective, since they require extensive labor for research and data-collection. Moreover, they are relatively slow means of analysis, so they cannot forecast (or nowcast) influenza trends as well as the other methods.

    While InfluenzaNet and Google Flu Trends are preferable in some respects, they each suffer some particular drawbacks as well. First, since InfluenzaNet bases its analyses on self-report survey data through European websites, it is susceptible to representation error through self-report bias. Additionally, since the survey is web-based, it excludes individuals without internet access, potentially skewing the results away from the population of interest. Google Flu Trends, since it is also based on internet activity, suffers similarly from this variety of representation error. Also, since Google Flu Trends analyzes flu prevalence based solely on raw search data, it is likely to overestimate the severity and spread of the flu in given areas. Not all flu-related searches come from individuals with ILI symptoms, but, according to the Google Flu Trends methodology, they would be counted nevertheless.

2. When trying to assess the outbreak and spread of a new strain (e.g., swine flu [H1N1]), each tracking method is susceptible to errors. With InfluenzaNet, the representation errors previously mentioned are potentially dangerous. Since its measurements rely solely upon responses to a web-based survey, it fails to account for individuals without internet access, who might be a particularly at-risk population (correlative with poverty, isolation, etc.). As such, InfluenzaNet might underestimate the severity of a strain by failing to source a representative sample of the population. 

    Google Flu Trends suffers from a similar error, since it also depends on internet activity. However, this method could potentially *overestimate* severity and spread by depending solely upon internet searches. As previously stated, not all searches pertaining to influenza come from individuals with ILI symptoms. This is especially true for widely publicized strains, like the swine flu. People trying to source information on their own about the severity and spread of a widely discussed influenza strain would be included in the count of those afflicted, according to Google Flu Trends. Inferences using this methodology are thus more likely to overhype the severity and spread of the strain in question, with potential consequences being the inappropriate deployment of resources or information.

    Finally, traditional methods of influenza tracking are susceptible to inaccuracies stemming from their slow method of analysis. Since these methods depend on careful virological research and direct interface with physicians, their conclusions are likely to be delayed by weeks, making accurate predictions more difficult to make. With delayed conclusions and thus treatment, more people might be put at risk. These methods, while potentially more insightful in the long run (through epidemiological research producing valuable knowledge about the virus), are also the most expensive. As such, fewer resources would be available for treatment and public information campaigns, potentially leading to a greater population risk. 

[comp]: https://www.kaggle.com/competitions
[kagglecomp]: https://www.kaggle.com/c/spooky-author-identification
[JPS]: http://journals.sagepub.com.proxy.uchicago.edu/home/pss
[paper]: http://journals.sagepub.com.proxy.uchicago.edu/doi/full/10.1177/0956797617709814
[IN]: https://bmcpublichealth.biomedcentral.com/articles/10.1186/1471-2458-10-650
[GFT]: http://science.sciencemag.org.proxy.uchicago.edu/content/343/6176/1203.full
