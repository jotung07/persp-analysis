Joseph Denby  
Dr. Benjamin Soltoff  
MACSS30000  
Final Paper  
December 6, 2017  


#### Edelman & Luca (2014)
---
1\. To begin with a brief summary, this study aims to probe the prevalence of discrimination on the crowd-sourced renting platform [Airbnb][airbnb] by computing and comparing average rental prices across racial categories. More specifically, this study takes all listings in New York City on July 17, 2012, matches listings along dimensions such as property characteristics (number of bedrooms, shared property, etc.), location, and guest reviews, and analyzes the impact of race (black versus non-black in particular) on listing price using linear regression. Their analysis finds that, on average, non-black hosts charge and earn approximately 12% more than black hosts for nearly identical listings.  

This research design benefits heavily from computational methods at various points, most notably with the data collection. As discussed in the paper, the entire dataset was cobbled together from variables scraped from Airbnb, specifically property information, host information (including profile pictures), rental price, and user reviews for all property listings in New York City on July 17, 2012. In total, this dataset included 3752 distinct hosts and listings; with their research design, the researchers employed this computational method to amass a significant amount of rich data at a reasonable cost (temporal and economic), leading to more robust analysis and, thus, conclusions. Also, in order to racially categorize hosts and quantify the quality of listings (as might be assessed by prospective lodgers), the researchers outsourced these trivial tasks to workers on Amazon Mechanical Turk. By looking to this massive platform for human computation, the researchers solve the individually simple yet collectively laborious task of cleaning this large dataset quickly and cheaply. 


2\. As previously stated, this study claims that discrimination is identifiable on Airbnb based on analysis finding that, on average, New York City non-black owners earn 12% more than black owners with similar properties. This study's research design employs computational methods to yield observational insights about preexisting data that would be otherwise difficult to glean. By using webscraping techniques and Mechanical Turk workers to collect and clean data, the researchers could easily match similar listings and differentiate the race of hosts, allowing for a robust investigation of the effect of race on price using a large and rich dataset. Put more succinctly, the researchers take advantage of Airbnb itself being a 'big' dataset that is, with the right tools, relatively easy to access at any time. 

However, this study's research design suffers from some drawbacks that hinder the strength of its conclusion. For one, the authors acknowledge that their design and dataset do not allow for the evaluation of demand effects as a vehicle for discrimination, primarily due to the inaccessibility of the relevant data. While price is certainly a component of discrimination in the marketplace, its effects are likely tied to demand effects (as a price discrepancy reflects lower demand for that specific listing, even if the only difference between it and a more expensive listing is the race of the owner); so, evaluating demand effects is key to providing a complete and thorough investigation of discrimination on the platform. Airbnb presumably has information that would lend itself to an evaluation of demand per listing (e.g., number of requests/messages, number of unique webpage visits, average time spent on that listing's webpage, etc.), but, as the authors state, the company declined to share any proprietary information with the researchers. Since this study's research question pertains (in part) to inaccessible observational data, its conclusions are not as robust. Next, this study's dataset is composed exclusively of listings within the New York City area on July 17, 2012. While this relatively circumscribed population may make matching and analysis a bit more tractable, any conclusions about discrimination on Airbnb suffer from the dataset's incompleteness. The study's findings about the price effects attributable to host race are certainly intriguing, but they cannot in good faith be made about the platform *as a whole* on the back of data sourced solely from one city on one day. In order to make a convincing claim about discrimination on the entire platform (or a population larger than just NYC listings), the researchers would need to scrape a more representative sample of listings from the entire population of interest.  

#### Edelman, Luca, & Svirsky (2017)
---
1\. As with the previous paper, this study looks to investigate the prevalence of racial discrimination on Airbnb. However, this study's research design employs an experimental methodology to answer this question. Specifically, the researchers created ten sham accounts with names evocative of a particular gender (male or female) and race (White or African American), sent inquiries from these accounts to the hosts of 6400 listings located in five cities, categorized the responses, and compared the proportion of "Yes" responses across the account conditions. In analyzing the data collected from their experiment, the researchers find that accounts with distinctly African American names receive 16% fewer "Yes" responses, an effect that is robust to controlling for location, price, type of property, and host demographics. Upon further investigation of the hosts' contacted however, the researchers found that discrimination was concentrated primarily among hosts with no African American guests among their previous ten guest reviews, suggesting that familiarity may play a role in mitigating discrimination.

In order to investigate discrimination on Airbnb's platform, this study employs computational methods at nearly every stage of the research process. To begin, sham Airbnb accounts were created with names that were meant to be clearly evocative of a particular gender and race combination. In order to arrive at such names, the researchers drew from previous work on the topic of name-based racial discrimination, and employed Amazon Mechanical Turk workers to verify the predictive characteristics of those names. Next, the researchers used custom-build scripts to automatically send inquiries from the sham accounts to nearly 6400 listings, collecting large amounts of data with relatively little effort. Finally, webscraping tools were used to collect data about the hosts' demographics and listing information, and Turk workers were again employed to categorize the hosts by race, gender, and age. By using computational tools at this step (including human computation), the researchers were able to amass, clean, and quantify large amounts of variable data, providing ample parameters with which to build a model that could provide the richest insights about the question of interest. This included using a specialized face detection API (Face ++) to categorize the race of every user included in each host's ten most recent user reviews, which allowed for the researcher's analysis that revealed the effect of renter history of discrimination.  

2\. This research design, as a carefully controlled experiment, employs computational methods to shrewdly pinpoint the hallmarks and potential causes of discrimination. Through this study, we learn the extent to which discrimination towards renters exists on Airbnb's platform, as well as the connection (or lack thereof) between discrimination and other variables of interest. The primary boon given to this experiment by computational methodology is its ability to assess the heterogeneity of treatment effects among various populations. Using webscrapers, the researchers could collect large amounts of data about each host and listing involved in the experiment, allowing for the divvying up of participants into different groups based on race, gender, age, and location, as well as listing price and prior user ratings. Through their analysis, the researchers found that the average 16% drop in positive response rate experienced by the African American-named accounts persisted across all populations except hosts with recent African American guests. By using computation to collect information about many variables, the researchers were able to isolate heterogeneity that might typically go unnoticed. As a result, we learn that hosts with recent African American guests (who left a review) did not respond differently to the African American accounts, potentially providing evidence for familiarity as a means of mitigating discrimination. 

As for validity, the experiment adequately mimics the typical experience of both hosts and prospective tenants on Airbnb, all while yielding informative results by effectively manipulating the hypothesized variables of note.   By using Amazon Mechanical Turk workers to verify that the sham accounts' names evoke specific race/gender combinations, as well as using scripts to generate and send believable message to hosts, the researchers ensure that the participants (hosts) are reliably placed in a discrete experimental condition in a way that does not smack overly of artificiality. (The sham accounts are slightly unlike real accounts in that they are devoid of information beyond a name, but the authors convincingly argue that the benefits of this degree of artificiality – namely the experimental control it affords – outweigh any minute costs to internal validity.) Moreover, since the only differences between the accounts are the names, there is no possibility for any confounding influence to affect the response variable of interest. Since the topic of interest is the prevalence of discrimination on Airbnb, and the experiment operates solely within the Airbnb marketplace, the study satisfies any criteria for external validity. Small points of contention may concern the participant sample – in order to yield conclusions that are robust to the Airbnb platform *on whole*, the participant pool should be as close to a simple random sample from among Airbnb's user base as possible. However, since the 6400 hosts contacted were spread across five relatively idiosyncratic cities (geographically, if not culturally), the results can be confidently taken as representative of the population of American users. 

The only major drawback to this particular study, as the authors acknowledge, could be its inability to exhaustively probe the potential causal mechanisms driving discrimination (or its mitigation) on Airbnb. The authors point out that while the bareness of the sham accounts allow for complete control over the experimental manipulation, it leaves out the possibility to investigate certain hypothesis about certain variables' impact on discrimination. Particularly of note, the authors bring up the potential for a *user's* prior reviews to influence a hosts' response to an inquiry. While the study's current findings indicate a discrepancy in positive response rate when comparing accounts with distinctively African American names to those with White names, as it stands it cannot evaluate the extent to which a given user's prior reviews might mitigate that discriminatory difference in baseline. Moreover, the authors acknowledge that their study does not robustly evaluate different models of discrimination, particularly the statistical and taste-based models. While this experiment yields fascinating insights about how discrimination manifests itself on Airbnb's platform, it does not contribute heavily to ongoing work concerning the root cause(s) of this discrimination or how it might be curbed most effectively. 

#### Together
---
1\. To begin, the two projects complement each other by addressing and investigating different aspects of the same research question. Specifically, the observational study investigates discrimination towards *hosts* on Airbnb by observing the effects of host race on listing price, while the experiment investigate the discrimination towards prospective *lodgers* by comparing the proportion of positive responses experienced by accounts with racially distinctive names. Because the two projects concern themselves with discrimination affecting the two primary categories of users on Airbnb, together they provide a more complete picture about the presence of discrimination on the platform as a whole. 

With respect to specific methodology, each project's research design contributes unique aspects to our understanding of the research topic. First, the experiment-based project is vital to our comprehending the specific effects of certain variables on race/gender-based discrimination. As previously stated, since the experiment uses artificial accounts to manipulate the race/gender variables, it ensures that there are no confounding mediators that are surreptitiously affecting discrimination. As such, its results can reliably attribute changes in discrimination (or lack thereof) to the specific condition in which the participants were placed; they can also investigate how this specific manipulation can yield different effects (or the same effect to different degrees) for different populations – assessing heterogeneous treatment effects would be impossible without an experiment-based research design.

On the other hand, the observational study highlights how discrimination affects *real* users instead of the artificial sham accounts that are arguably unlike actual users' accounts. By investigating discrimination using data sourced from contemporary interactions between actual users, an observational design maximizes validity with respect to the inferences made from the data and how they pertain to current users. After running both an observational study and an experiment, we learn about discrimination at multiple levels of granularity: we gain an understanding of how discrimination, broadly defined, affects actual users as they use they currently use platform, and, through experimentation, we learn exactly what parts of that observed discrimination are attributable specifically to race and gender, how discrimination is mediated and mitigated by other variables, and how pervasive race/gender-based discrimination is across various subpopulations.

2\. A potential issue with both of the research designs currently proposed is that they both depend on a proprietary measure of discrimination (based on mean listing price or proportion of positive responses). However, discrimination is arguably something that is most acutely experienced at the individual level – so, in order to faithfully track the experiences of Airbnb's user base with respect to discrimination, a survey may provide a great means of sourcing that information directly from users. I would propose sending a survey to a random sample of Airbnb users that asks the respondent to rate their experiences with discrimination using a series of Likert scales. I am hesitant to propose any specific questions before consulting with a communications or survey expert; in order to avoid any extraneous language or priming effects I would want to use neutral phrasing for the survey questions. In addition, I would counterbalance question order between respondents, so as to avoid order-based priming effects.

Sourcing discrimination data directly from Airbnb users would be an excellent way to understand how the userbase genuinely feels, but, in order to receive a sample that reflects the platform's average sentiment regarding discrimination, steps need to be taken to avoid representation error. With the content of this survey in particular, there might be a problematic link between response probability and the outcome measure. Specifically, individuals who *have* experienced discrimination on the platform may be more likely to respond to the survey than those with a relatively uneventful Airbnb experience; if this were true, the survey responses would overestimate the level of discrimination present on the platform, leading to inaccurate conclusions. To correct for this potential link between response probability and outcome measure, some sort of response incentive could be put in place to ensure that all recipients, regardless of their reported experience, would be approximately equally as likely to respond. With an incentive in place, the survey pool would have a higher response yield, making the random sample's statistics more representative of the population parameters of interest. After the most pressing areas of concern – the potential for representation error and extraneous priming/order/language effects sourcing from the survey itself – through careful survey design and the implementation of a response incentive, the survey stands to reveal uniquely useful results about the topic at hand. 


[airbnb]: https://www.airbnb.com