---
title: "Final Exam"
author: "Lerong Wang"
date: "12/06/2017"
---

## Introduction

Online marketplaces nowadays contain more and more information that does not limit to the product itself. More information about the seller is being provided. Information about seller is designed to facilitate trust between buyer and seller, but may also raise questions about discrimination. Both of the two papers use Airbnb as an example to investigate the extent of racial discrimination, but they measure the effects of racial discrimination by evaluating different constructs. 
 

## Racial Discrimination in the Sharing Economy Evidence from a Field Experiment. 
### Research design

Airbnb has a feature that allows guests to contact the hosts about the availability by clicking the “Contact” button and write to the hosts. The study in this paper took advantage of this feature and conducted a digitally-enhanced field experiment. Researchers used the frequency of names from birth certificates of babies born between1974 and 1979 in Massachusetts to find evident names of African American males, African American females, white males, and white females. These four categories constitute the four main treatment groups (2017). 20 Airbnb accounts were created with different names from the name list researchers created, and all other aspects were the same. These 20 accounts were used to act as fake Airbnb guest and messages were sent to prospective hosts from these accounts using the contact feature. For hosts with multiple listings, researchers made sure that they only received the messages once. In order to avoid further discrimination and potential bias, the 20 accounts did not have profile pictures. The messages sent out from the accounts asked the hosts about the availability of the listings on a weekend that was about eight weeks distant from the message was sent. Roughly 6400 messages were sent to hosts in between July 7, 2015, and July 30, 2015. Researchers tracked response over 30 days and recorded the responses based on different categories. Absolute “Yes” and “No” responses were considered in this experiment and further analysis. After researchers gathered the results and analyzed the potential effect of racial discrimination, they further explored and extended their research by combining observation studies and checked the most recent reviews of the hosts to see if they have received at least one review from an African American guest. 

### How the research design leverages computational methods

During the sample and data collection process, researchers saved the hosts’ profile pictures and recruited participants from Mechanical Turk to identify the race, age, and gender of the hosts. Two workers were hired initially, and if they disagreed, a third worker would be hired. If there’s still disagreement, then researchers manually coded the picture. Face++ was used to categorize the race, gender, and age of past guests who had previously reviewed the hosts. 
The whole experiment was conducted without actually interacting with the participants face-to-face, and took good advantage of the existing feature of Airbnb without interfering the normal use and discipline of Airbnb. All data was collected using scrapers, and the inquiries to Airbnb hosts were sent using web browser automation tools. 


### Effectiveness

I think the experiment in this paper is reasonable and effective. The researchers did take as many confounding factors as possible into consideration to justify their results. For example, when I started reading this paper, I had a question about in-group bias, and I was wondering if the racial distribution of Airbnb hosts in this study would contribute to different results. While continuing reading the paper, I found that in-group bias has already been taken into account by presenting regressions that include guest race, host race, and an interaction term. Other important confounding factors such as the bare use of guest profile and host’s proximity to the guest were also taken into consideration. 
I think this experiment has strong internal validity. Since some hosts have multiple listings, researchers used a random number generator to select only one listing per host, which minimized possible confounding factors. Each host was randomly assigned to receive the message from one of the 20 accounts. The randomization process and delivery of treatment were all conducted reasonably. Construct validity was also satisfied in this experiment. Ultimately, the study wanted to measure the effect of racial discrimination on Airbnb. The effect of racial discrimination was materialized reasonably by evaluating a host’s receptiveness to a booking from a given guest. In the article, the author mentioned assessing the external validity by assessing the ten most recent reviews on each listing page and checked whether the hosts had at least one African American guest review. They found that the gap drops sharply among hosts with at least one recent review from an African American guest. This process to assess the external validity sounds reasonable, but I think given the scope and depth of their observation on guest reviews, and the sample size of this experiment compared to the size of all users on Airbnb, the evaluation of external validity may be questionable to me at this stage 
The research design made good use of the big and non-reactive nature of big data. Unlike traditional experiments that researchers would recruit participants, this digital experiment used the existing feature and user profile of Airbnb. The sample size for this experiment is big enough for researchers to make inferences because 6400 messages were sent to randomly selected hosts. The non-reactive nature of digital data is also satisfied, which is also different from the traditional lab experiment. Hosts did not know they were being involved in a study, so their reaction can truly reflect their attitudes. However, the design also inherited some bad features of big data. Since the data were collected from Airbnb in 5 metropolitan areas, we don’t know if the data will be representative if we take all Airbnb in the US into consideration. 



## Digital Discrimination: The Case of Airbnb.com
### Research design

The goal of this study is to investigate the extent of racial discrimination against hosts on Airbnb by empirically investigating the differences in prices between hosts of different races. In this article, researchers mainly consider the comparison between black and non-black hosts (2014). The data was collected from all New York City landlords on Airbnb as of July 17, 2012. Researchers made their own dataset including a snapshot of listings contained on Airbnb, price, characteristics of the host and characteristics of the apartment. Workers were hired to rate the listing’s photos and categorize the hosts into different races. Researchers compare the price gap between black hosts and non-black hosts, and some other important aspects that may affect the change of price were controlled. Regression and t-test statistics were used in the analyzing process. 


### How the research design leverages computational methods

Researchers hired two groups of workers from Amazon Mechanical Turk. The first group is hired to examine each listing’s photos, and participants were asked to rate the quality of each listing based on the photos of the listings on a seven-point scale. 

### Effectiveness

This observational study takes advantage of digital data. Researchers gathered data and made their own dataset by using the accessible data on Airbnb. The study demonstrated the non-reactive and always-on features of big data. With the non-reactive feature, both hosts and guests don’t know that there are researchers conducting the study on Airbnb and some of their behaviors will be utilized, so Airbnb users will not change their behaviors. Thus, the information researchers gathered from the website can reflect the true behaviors of the users. The always-on feature enables researchers to conduct a longitudinal study on this topic so that they can compare the changes over time, especially when the price of a listing is changing. However, this classic observational study using big data can also lead to some controversial issues. First of all, the data is incomplete and somewhat inaccessible. The author mentioned at the beginning of the article that ideally their analysis would consider both price and demand effects. However, due to the limited information, they can directly gather from Airbnb website, and Airbnb’s unwillingness to provide more data, this paper lacks the analysis of consumer demand. The data may not be representative. The dataset only considers Airbnb of the city of New York, which may not be representative of the Airbnb data in the US， since New York City is an especially popular tourist attraction. Also, the time period that the data set was collected may also result in different prices and price gaps. Drifting is also an important issue that researchers don’t have control over but still needs to be considered. The price of the listings may be changing over time, and different hosts may change their prices of the listings at a different time. The policies of Airbnb may also be changing over time. For example, Airbnb may come up with a new policy regarding price adjustment. The data set is also dirty. Since Airbnb does not have information about hosts’ racial status, researchers have to hire participants from Amazon Mechanical Turk to code the race of the hosts. However, this may be a subjective process and can lead to errors. 
Controlling confounding factors is a big challenge and also a potential limitation of this study. Since researchers gathered raw data directly from Airbnb, they had to use statistical methods to control possible confounding variables. However, researchers still cannot eliminate the effect of this possible confounding factors.


## Identify the value-added of conducting both research projects.

The two papers that I discussed above each have their advantages and disadvantages. By conducting an observational study, the non-reactive nature allows researchers to assess the raw data which reflects the reality. However, raw data is also dirty. Though researchers utilize statistical methods to reduce confounding variables, the results, to a certain degree, definitely can’t get rid of the influence of confounding variables. For example, in the second study I discussed above, researchers hired workers on Amazon Mechanical Turk to rate the quality of the photos of each listing on a seven-point scale so that researchers can compare the prices between black hosts and non-black hosts within the same photo quality category. However, this is a subjective process, and the criterion of what indicates a good quality differs among different people. Thus, though the researchers try their best to eliminate the effect of quality of the listing’s photos on price, the effect may still persist inevitably. However, by conducting an experimental design, since the researchers have control over dealing with confounding variables, the effect of other factors that affect the measurement of our constructs will be minimized. The first study uses a field experiment, which may more or less suffer from the loss of authenticity and researchers' confirmation bias. By combining both experimental design and observational study, the research design will be more powerful. Actually, the first article I discussed above is involving mostly experimental design but also with some observational aspects. Combining experiment and observational method, we can access more real-world data and at the same time, minimize the effects of confounding factors. In the digital age, more digitally-enhanced study involving online platforms are being conducted with the permission of the online platforms, so if we can partner with Airbnb and get its permission to conduct a study, we can largely improve the original research design of both studies. By using these real-world data for field experiment, we are increasing both the strength and scope of the research design. 

## Consider how you could apply a digital survey-based research design to the primary question of interest from these two papers. What are the potential drawbacks to a survey approach? How might you overcome these drawbacks?

Both the two papers attempted to investigate the extent of racial discrimination on Airbnb. For the observational study that measures the price gap of black and non-black hosts, I think it's hard to directly come up with a survey study also measuring the price gap of different hosts, since participants may have different economic status, and their perception of how expensive the apartment should be will vary largely. To add to the experimental study, I will create a survey targeted at choosing hosts. Participants will be recruited using Amazon Mechanical Turk. Since I don’t want the participants to know the true intent of the survey, I will create a few irrelevant questions to minimize the potential bias of the survey. To begin with, there will be some demographic questions:

1. What’s your age?
2. What’s your gender?
3. What’s your race?

To make the survey more ethical, participants may choose the option “I don’t want to answer this question” on these demographic questions. Then there will be some irrelevant questions such as “If you have enough money to travel, what will be your destination” that they have to select a certain answer to filter out some unconscientious response. In between these irrelevant questions, related questions about choosing Airbnb hosts will appear. The hosts on the screenshots I provide will have similar price, location, apartment type, but with profile pictures showing their different races. Sample questions are shown below:
1. On a scale from 1 to 5, how likely will you choose this apartment? 

Survey study is good at controlling potential confounding variables, but it also has some obvious drawbacks. Participants may not fill out the survey conscientiously, or they may not provide honest answers. The data may also be incomplete. For ethical reasons, the way I designed my survey gives participants choices over whether they want to answer this question or not. This kind of research design can avoid some ethical issues, but it also leads to an incomplete data set, which may produce data errors. The dataset will be dirty, and researchers need to delete useless data from the dataset manually. Moreover, participants may have a sense of the goal of the survey, so in this case, they intend to avoid their real attitude.


To overcome these drawbacks, I first need to guarantee that my sample size is large enough so that if I delete some useless data, I would still have a large enough sample size. Next, I will make my questions simple enough so that participants will not feel time-consuming. Moreover, I will also make some questions asking participants to choose certain answer to filter out the unconscientious response. Finally, I can also ask some indirect or irrelevant questions to avoid the bias resulting from participants knowing the possible goal of the study.


## References

“Bit By Bit: Social Research in the Digital Age.” Bit By Bit: Social Research in the Digital Age, www.bitbybitbook.com/.

Edelman, Benjamin, et al. “Racial Discrimination in the Sharing Economy: Evidence from a Field Experiment.” American Economic Journal: Applied Economics, 2017.

Edelman, Benjamin & Luca, Michael. "Digital Discrimination: The Case of Airbnb.com." Harvard Business School, 2014


