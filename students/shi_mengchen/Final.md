# Perspective on Computational Analysis
## Final Exam: Evaluating Research Designs
#### Name: Mengchen Shi
#### Email: mshi17@uchicago.edu
#### Date: December 6th, 2017

## The 2014 article: Observational Study
### Summary of Research Design and How They Leverage Computational Methods
Edelman and Luca (2014) conduct an observational study to test for racial discrimination against landlords in the rental marketplace Airbnb.com. Their research strategy is approximating a matching experiment. Specifically, they use a data set combining pictures of all New York City landlords on Airbnb as of July 17, 2012. with their rental prices and information about quality of the rentals, to empirically investigate the price gaps between black and non-black hosts. Their data, including the asking prices, characteristics of the host and the apartment, the number of left reviews, and average ratings of the property, are collected from the public information on Airbnb. They also hire workers on Amazon Mechanical Turk to exam photos of the listings and to identify the race of the hosts on Airbnb according to the public profile pictures of the hosts. They first find that non-black and black hosts receive strikingly different rents by exploring raw data. Then they run a regression of asking prices on race of the hosts and find that non-blacks earn roughly 12% more than black hosts with other information equivalent. They confirm robustness by controlling other attributes observable on Airbnb and by analyzing other racial differences in outcomes. However, they cannot figure out if the differences come from tasted-based discrimination or statistical discrimination.

Apart from using a matching experiment, they also take advantage of mass collaboration, specifically, human computation. The problems to identify races of hosts on Airbnb are split into little problem chunks. Then human workers on Amazon Mechanical Turk are involved to solve each little problem chunk, i.e., to look at the profile picture of a host and identify the race of him/her. Finally, the results are combined to serve as the most important variable in this study. This approach is a typical split-apply-combine strategy. They also apply the same strategy to determining the quality of the apartments according to pictures on Airbnb. 

### Effectiveness and limitation of the Article
Edelman and Luca (2014) take advantages of three good characteristics of big data mentioned by Salganik(2016), while there are some limitations due to the bad sides of big data. 

First, the research benefit from a big volume of data. As the one of the most popular short-term rental marketplace, Airbnb has a great number of listings and users, which provides sufficient and representative data for analysis. However, they could have utilized the bigness of data better. For example, they only investigate listings in NYC, which too special to be representative for the whole environment on Airbnb. They could have investigated more cities in the U.S., including region as a variable or analyzing discrimination by regions. Besides, they could have collected data during across different periods in a year instead of collecting data of a single date. By collecting dates and data from more regions, they can construct panel data to drive more convincing results. 

Second, the “always-on” characteristic of big data benefits this research by providing data about the hosts over long time. Specifically, the reviews of a listing in the past are available to the researchers, serving as important variables to the analysis. However, the researchers do not fully utilize the “always-on” characteristic. A single is too special to represent the whole price level. As mentioned above, they could have observed the hosts in a long period, recorded volatility of the asking prices, and generated a more representative data set. 

Third, the research avoids reactive problems with accessing big data. The hosts are not aware that their behaviors are observed since the data crawling methods do not disturb them directly. Therefore, their observational data are more reliable.

This research suffers from some negative properties of big data. 

First, the data are incomplete. The researchers lack information about hosts’ accurate age, which is a basic demographic variable for researches of this kind. 

Second, some data are inaccessible. Ideally, the research should consider demand effects that directly reflect the popularity of a host. As mentioned in the paper, Airbnb was not willing to share such data, which forced the researchers to focus on only pricing. Moreover, if data were accessible, researcher could have analyzed prices in the whole marketplace from the very beginning, which will help to explore the formation and the causes of the price gaps between races.


## The 2017 Article: Experiment 
### Summary of Research Design and How They Leverage Computational Methods
Edelman and Luca (2017) run a field experiment to test for racial discrimination against guests in the rental marketplace Airbnb.com. Specifically, they inquire about the availability of about 6,400 listings on Airbnb of July 2015 across five cities by creating guest accounts that differ by name with other information identical. Their names include then distinctively African American names (5 male and 5 female) and ten distinctively white names (5 male and 5 female). They send messages by these accounts to random selected hosts on Airbnb to ask about availability of the apartments and tracked responses from the hosts. Their data about listings and hosts on Airbnb are collected similarly to the 2014 paper, except that they include past guests of the listings. They cross-validate the findings using observational data on whether the host has recently had an African American guest. By running a regression of the host accepts rate on race of guests, they find that guests with white names are more likely to be accepted than guests with African American names by 8%. Moreover, they analyze effects by host characteristics (gender, race, etc.), listing characteristics, names and past reviews, confirming the effect of the main result. Furthermore, they estimate the cost of refusing a guest due to race. 

This study involves not only experiment but also observational methods, so its computational methods are similar to their former study in some way. First, they use the split-apply-combine strategy. They employ MTurk workers to assess race, gender and age of hosts according to the hosts’ images. Each image is assessed by two workers, and it will be assessed by a third worker if there is a disagreement, which generates more high-quality results. These data serve as critical control variables as well as to explore differences in discrimination by group. Moreover, to explore if discrimination is associated to race of a host’s past guests, they utilize a face-detection API to categorize past guests by race, gender, and age, which confirms the external validity of the result. Finally, they run the experiment using existing environment, namely, Airbnb, which is of low cost and high realism. To collect all data, they use web browser automation tools (scrapers) and send inquiries to Airbnb hosts for this purpose.

### Effectiveness and limitation of the Article
This paper concentrates more on experiment than on observational study. But undoubtedly, this digital enhanced experiment does benefit from big data as well. Similar to the former study of 2014, it takes advantages of bigness and “always-on” of big data, as well as suffers from incompleteness and inaccessibility of big data.  It overcomes some limitations of the former study by including more cities. However, by conducting an experiment, it addresses some ethical problems by sending fake messages with fake accounts, which forgoes “non-reactive” advantage of big data. 

As an experiment, its validity, heterogeneity of treatment effects and mechanisms requires close evaluation.

First, the design of the experiment ensures that treatment is delivered correctly, which reduces concerns about internal validity. But as the created accounts for experiment were blocked eventually, it is hard to say that all their messages sent to hosts were received and all the responses were received by the researchers. If this is the case, the internal validity is not valid. The researchers use reviews from previous guests and their profile pictures to assess the external validity of the study. That is, they can tell which hosts previously accepted African American guests, and find that discrimination disappears among those hosts who have previous African American guests. As for construct validity, the study does not provide answers to the question what kind of race discrimination it is – taste-based or statistical discrimination often distinguished by the theoretical literature. Besides, they fail to clarify the mechanism of this discrimination, so we are not convinced that the observed discrimination is race discrimination even though it seems to be caused by race.

Second, the design of the experiment fully assesses the heterogeneity of treatment effects. They estimate the treatment effects by running regression with control of host characteristics (gender, age, interaction term, proximity to the guests, and professional/occasional property, etc.), and find the race discrimination persist. They estimate treatment effects by running regression with control of listing characteristics (price, eventual fill, diversity of neighborhood, etc.). The also assess the robustness of the treatment effects by names of the fake accounts. 

However, the research has important limitations about mechanisms. Firstly, it fails to identify whether the discrimination is based in race, socioeconomic status or a combination of these two. Similarly, their experiment does not provide a sharp test to distinguish the observed discrimination between statistical and taste-based discrimination, even though they did try and found a more nuanced story than either of these classic models.

Lastly, the experiment brings up some potential ethical concerns. First, even though the researchers sent inquiries to Airbnb hosts using web browser automation tools, they did not ask for consent in advance, nor did they inform hosts of the experiment. Second, by sending fake messages, the researchers deceived the hosts to some degree and might have caused their loss. For example, if a host says “yes” to the fake guest and refuses a genuine guest, the host will suffer from loss. Moreover, the fake users might occupy rental choices for genuine users, which harms the genuine users. Obviously, the two cases above will both harm the interest of Airbnb.

## Value-added of Conducting Both Research Projects
The observational study and the experiment bring evidences that there exists race discrimination on Aribnb, but from two different perspectives. The observational study reveals the race discrimination against hosts on Airbnb, while the experiment proves that race discrimination against guests exists as well. Each of them cannot address a convincing and complete story alone, but the combination of them makes the argument much more powerful. 

One could argue that they can search for evidences about race discrimination against hosts and guests by doing only observational study or by only conducting experiments. But it requires more complicated design, if possible. For instance, it is hard to estimate race discrimination against guests by pure observation. First of all, the data of guests’ inquiries and the responses they receive seem inaccessible to the researchers. Airbnb will probably refuse to provide these data since they contain sensible business messages and private information about their users. Not to mention that Airbnb will not be willing to provide data to a research that will probably drive negative conclusions about Airbnb. This guess is reasonable as neither the papers reveals any evidence that they have received any support from Airbnb. Hence, conducting an observational study to analyze race discrimination against the guests is impossible. Similarly, it is not easy to study race discrimination against the hosts by conducting an experiment. Some people might suggest creating fake host accounts on Airbnb, uploading fake profiles, and listing fake apartments to attract guests. While it seems conductible, it will require much more efforts to create a fake host than to creating a fake guest – Airbnb has a strict process to confirm authenticity of a host than a guest. Even worse, even if accounts of fake hosts are created successfully, this operation might cause law troubles because of “deceiving customers”. Moreover, controlling factors other than race to be identical is complex. For instance, to create four hosts (male, female, black, non-black) with other characteristics identical needs four fake accounts, to create hosts with a different permutation of characteristics needs another four accounts, which requires a great deal of work that could be avoided by conducting an observational study instead. 

Moreover, the experiment suffers from the internal validity issue as I mentioned above, while observational study does not suffer from this issue.

Lastly, the experiment can generate causal inference because the mechanisms are designed by the researchers. They can freely change the properties of their fake guests and find out outcomes accordingly. While observational study can only observe the existed data and have little ability to control the factors to observe why the average pricing are lower among black hosts.

## Digital Survey-based Research Design
The primary question of interest from these two papers is whether there exists race discrimination on Airbnb. To conduct a survey-based research, we should separate the survey into two parts as these two papers: one survey to ask hosts about their responses to guests of different races, and the other survey to ask guests about their choices of listings owned by hosts of different races.

Specifically, the survey to ask hosts about their responses to guests of different races should include four-choice questions: “No”, “Yes”, “Request for more information”; “Yes, with questions” (if the host approved the stay but also asked questions). For each question, the participant (the host) is presented with a snapshot (from Airbnb.com) of profile of a guest that indicates race and a rental inquiry from that guest. The analysis of the survey outcomes is similar to that of the experiment outcomes in the second paper. 

The survey to ask guests about their choices of listings owned by hosts of different races should also include choice questions. Each question offers several snapshots of profiles of potential hosts that indicating different races and their listing of apartment for rent. Some of the questions present identical apartments with the same price for hosts of different races, and some of the questions present identical apartments with different price for hosts of different races. The participants (potential guests) are asked to choose their favorite listing for one question. This method evaluate demand instead of asking prices, which is a good complement to the observational research conducted in the first paper.

A direct and ideal way of conducting this survey is to deliver questionnaires on Airbnb. Unfortunately, as mentioned in the former part, Airbnb is unlikely to offer support to this research. An alternative is to distribute surveys according to internet users’ digital traces. That is, to distribute questionnaires to those who have logged in Airbnb. But such a method might cost much more than a research can afford, and might raise concerns about breaking laws by collecting private data without consent. Some people might suggest distributing questionnaires by simply putting our survey on social networking services website such as Facebook, and attracting participants by money. Another alternative is to hire workers on Amazon Mechanical Turk and pay them for the survey. While these two approaches will potentially motivate enough participants, they cannot solve the representation error. That is, those who do the survey might need money because of poverty, which means they cannot represent the population on Airbnb that we hope to study. 

Another considerable drawback of the survey is that participants might become aware of the goal of the survey, and hide their true thoughts by choosing a “politically correct answer” accordingly. Their motivation to hide their true thoughts could be so strong that the outcomes of the survey could result in nothing but trash. To avoid such a problem, we could elaborate on designing a better questionnaire to hide out intention. For example, to make questions comparing various characteristics (prices, location, decoration, cleaning fees etc.) of listings be in majority of one piece of a questionnaire, and only a few questions comparing races are included. Another example could be not presenting the direct sign of race, namely the photos, but to provide names indicating races indistinctly.

Measurement error is another potential drawback of the survey. That is, what the respondents say is different from what the respondents think and do in the reality. To overcome this problem, one possible solution is to asking the respondents accessibility to their transaction data on Airbnb. By comparing their rental histories  on Airbnb (to whom they rented their properties, what kind of listings were viewed, and from whom they rented an apartment) and their answers to the survey, we can estimate a difference between responses and reality, and then take this difference into consideration when analyzing the outcomes. 

### References:
Edelman, Benjamin G., and Michael Luca. "Digital discrimination: The case of Airbnb. com." (2014).

Edelman, Benjamin, Michael Luca, and Dan Svirsky. "Racial discrimination in the sharing economy: Evidence from a field experiment." American Economic Journal: Applied Economics 9, no. 2 (2017): 1-22.

Salganik, Matthew J. 2017. Bit by Bit: Social Research in the Digital Age. Princeton, NJ: Princeton University Press. Open review edition.
