# Perspective Analysis HW5 - Collaboration
## Kaggle open call projects
### 2. Describe an open call project  -- Titanic: Machine Learning from Disaster (https://www.kaggle.com/c/titanic)

The goal of the competition is to explore the factors that increase the probability to survive in the Titanic and construct a machine learning model to predict which passengers survived in the tragedy. In order to make a submission, first we need to deal with the missing data, we may choose to either replace the missing value or delete the observations, in this case, replacing with a sensible values makes more sense as the task is evaluated by the accuracy of our prediction of survival status. Then, we need to explore the factors that have a correlation with passengers' survival, we may want to make some plots for a better present the patterns for the dataset. Next, we construct a binary classification model using machine learning techniques to predict passengers' survival, for example, the random forest method. The assessment of the model can be conducted by calculating the percentage of passengers whose survival status is correctly predicted by the model. We can compare our model with a benchmark model and investigate if there is any improvement can be done.  

## Improve a journal article 
### The Untold Story of the Clones: Content-agnostic Factors that Impact YouTube Video Popularity

The paper intends to identify the content-agnostic factors that affect the video popularity by controlling the content-related variables. To achieve this, the paper defines video sets that have the same content and audio soundtrack as clone sets. Then, as is put by the authors, "through extensive exploration, search and viewing of Youtube videos, we manually identified 48 clone sets, each of which contains between 17 and 94 clones, with a median size of 29.5.1 In total, we identified 1,761 videos." The identification of clone sets in the article is easy but requires tremendous effort and time given the large amount of data. Hence, modifying it to a human computation project can substantially lighten the workload of researchers. The implementation is relatively simple: first, we provide a definition and an example of clone sets for the hired workers. In order to check the workers' understanding of the task, we further provide a sample video and 5 similar videos (could either be defined as clone videos or non-clone videos). The workers are asked to determine whether each of the 5 videos belongs to the clone set. Compare the answers of the workers with the answers of the authors. If the workers pass the test, we assign them with a specific topic or a keyword and direct them to Youtube (some of the workers might not be provided with a specific topic, they are asked to randomly search videos on Youtube so that we can expand the topics and the amount of data obtained). The workers were asked to enter the URLs of the clone videos in the web-based collection system (developed by the original authors). Multiple workers might be asked to investigate the same topics/keywords. By doing so, we can collect a relatively larger number of videos in each clone sets, since individuals might find some clone videos that are not found by others. Furthermore, we can examine if the videos in our dataset satisfy the definition of clone sets: if a video is recorded more than once, then we are more confident with the reliability of the data. 

To re-emphasize, the human computation method can benefit the journal article in two ways: first, it can relieve the researchers from trivial but heavy work of data collection; second, the wisdom of crowds may increase the number of data, both in terms of the number of videos in each clone set and the number of clone sets with different topics. This improvement may lead to a more reliable result of the paper: the increase in the data per clone set may add precision to the estimation of regression coefficients, and the wider topic coverage tends to eliminate some unobservable confounding factors.  

## Influenzanet
### Compare and contrast
#### General description of the three systems 
Influnenzanet is an internet-based system that monitors influenza-like illness (ILI) in cohorts of self-reporting volunteers in European countries since 2003. Influenzanet participants are recruited from the general population by completing an intake questionnaire on one of the national websites, containing various demographic and life style questions. During the influenza season, participants receive a weekly newsletter by e-mail in which they are directed to an online questionnaire about a number of symptoms that they might have experienced since their last report. As the weekly data is directed collected from volunteers, there is no cost in hiring participants, hence the total cost is considered to be low for Influenzanet. 

The traditional influenza tracking system is generally based on the reports made by sentinel General Practitioners (GPs). The clinical surveillance of influenza in European Influenza Surveillance Network (EISS), coordinated by the European Centre for Disease Prevention and Control (ECDC), records the the ILI incidence as the number of patients who visit their GP and fit the (country-specific) ILI case definition divided by the total number of people assigned to the participating GPs. The U.S. version, Centers for Disease Control and Prevention (CDC) has an influenza monitoring system that collects information from doctors and reported fortnightly. The cost of the traditional tracking system is considered to be the highest among the three systems. The reason is as follows. The data from numerous healthcare service centers have to be collectively processed to satisfy a uniform standard before being released. Since the clinical practice is different between countries, the data collection and processing inherited incurs human and material resources.   

Google Flu Trends (GFT) is a web service that uses Google search queries to predict influenza-related activities for more than 25 countries. Given the data is internal, the major cost of the system is on model construction and the improvement of the algorithm.   

#### Potential Errors
The major error of the Influenzanet is the credibility of data sources. The first concern is selection bias. According to Tilston et al. 2010, the sample from Influenzanet is not demographically or geographically representative in the 2009 H1N1 UK influenza pandemic. Since people volunteer to participate and report their health conditions, it is reasonable to suspect that participants with certain unfavorable health conditions, for example, individuals already have ILI or are susceptible to ILI, are more likely to register. This bias might lead to an over-estimation of ILI incidence for the entire population. Meanwhile, the participants are required to complete a questionnaire that involves self-assessments of some ILI symptoms, as the participants are not required to have clinical-related knowledge, the credibility of the self-reported results is challenged. Furthermore, like the problems widely observed in survey research, the potential reactiveness of participants might cause measurement error. In addition to data credibility, the data is released one week after collection, thus, estimation of the outbreak of ILI in a short time might suffer from the lagged data.  

The failure in forecasting the influenza of GFT is partially caused by the algorithmic confounding. The Google engineers made changes to the search algorithm across the time in order to pursue profit. This change leads to unstable reflection of the prevalence of the flu. Meanwhile, relying on the search queries as the only model input, GFT is unable to capture the population drifts. For example, the changes of search behavior in response to widespread fear of global pandemic tend to be the reason for over-estimation of GFT during  2009 Swine Flu outbreak. Furthermore, according to Noort et al. 2015, the search of terms that correlate with the propensity of the flu does not predict future influenza events well. This suggests that with an enormous amount of data, the model is overfitting the small number of cases. 

The problems of the traditional influenza surveillance systems are 1. lagged data; 2. inconsistency of the definition of ILI across different countries; 3. the incidents only include individuals who contact healthcare services. For the first problem, like mentioned in the influenzanet part, the traditional surveillance systems might fail to predict the outbreak of ILI as the there is 1-2 week reporting-lag for both EISS and CDC. The second problem arises when we want to conduct a multi-country analysis of ILI. The reported ILI incidence is not directly comparable for different countries if the clinical practice is different in the first place. The third problem is believed to be the most important error in this method, for the data itself systematically omit the people who do not seek healthcare services. As a consequence, the estimation of overall attack rate of the entire population is likely to be downward biased. 

#### Possible errors during swine flu outbreak 

The GFT dramatically over-estimated the amount of influenza during the swine flu outbreak, one possible error arises from the population drift. The population changed their search behavior under the fear of a global pandemic, while GFT failed to make a response to these changes. These changes in the behavior might also affect the influenzanet, however, the direction is unclear since we are not sure about the psychological effect of a global pandemic on self-reported symptoms of ILI. The data for influenzenet is not geographically and demographically representative of th entire population, hence, adjustment with appropriate weights has to be done in order to eliminate the error. Restricted by the data reporting schedule, the influenzanet, and the traditional influenza surveillance system both suffer from the lagged data. Instead of reporting real-time incidents, both of the two systems report the amount of influenza one or two weeks ago. As mentioned in the previous section, the most salient error of traditional influenza surveillance system is that it only records the individuals who seek healthcare services. The omission for those did not seek treatment causes downward biases of the total amount of influenza. 


###reference

Borghol, Youmna, Sebastien Ardon, Niklas Carlsson, Derek Eager, and Anirban Mahanti. "The untold story of the clones: content-agnostic factors that impact YouTube video popularity." In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 1186-1194. ACM, 2012.

Tilston, Natasha L., Ken TD Eames, Daniela Paolotti, Toby Ealden, and W. John Edmunds. "Internet-based surveillance of Influenza-like-illness in the UK during the 2009 H1N1 influenza pandemic." BMC public health 10, no. 1 (2010): 650.

van Noort, Sander P., Cláudia T. Codeço, Carl E. Koppeschaar, Marc Van Ranst, Daniela Paolotti, and M. Gabriela M. Gomes. "Ten-year performance of Influenzanet: ILI time series, risks, vaccine effects, and care-seeking behaviour." Epidemics 13 (2015): 28-36.
