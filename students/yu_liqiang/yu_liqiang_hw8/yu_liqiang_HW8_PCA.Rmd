---
title: "yu_liqiang_hw8_Unsupervised_learning"
author: "Liqiang Yu"
date: "11/29/2017"
output: github_document
---
##Colleges
### Problem 1

Perform PCA analysis on the college dataset and plot the first two principal components. Describe the results.\

  (1) What variables appear strongly correlated on the first principal component?\
We can easily see from the graph that perc.alumni, Outstate, Grad.Rate, Room.Board, Expend, Top10perc, Top25 perc, PhD and Terminal are highly correlated on the first principal component.\

  (2) What about the second principal component?\
Variable highly correlated are, according to PC2: P.Undergrad, F.Undergrad, Enroll, Accept and Apps.
```{r}
library(dplyr)
library(ggfortify)
college = read.csv("/Users/Rex/Desktop/Perspective/HW/persp-analysis/students/yu_liqiang/yu_liqiang_hw8/College.csv")
head(college)
college_tidy <- college 
#Remove non-numeric data
college_tidy$Private <- NULL
college.PCA <- prcomp(college_tidy,scale. = TRUE, center = TRUE)
autoplot(college.PCA, data = college, 
         loadings = TRUE, loadings.colour = 'violet',
         loadings.label = TRUE, loadings.label.size = 3)+labs(title="First two principal components of college data")

```

### Problem 2
Calculate the cumulative proportion of variance explained by all the principal components (see 10.2.3 in ISLR). Approximately how much of the variance in College is explained by the first two principal components?\

From the PCA model, we conclude that the first two principal components comprise 58.36% variance of College data.
```{r}
summary(college.PCA)
vars <- apply(college.PCA$x, 2, var) 
props <- vars / sum(vars)
cumsum(props)
plot(cumsum(props),main="Cumulative proportion of variance",xlab = "PCA Index", ylab = "% of cumulative proportion of variance",type="b")
```

#Clustering states
###Problem 1

Perform PCA on the dataset and plot the observations on the first and second principal components. 

**Notice that for part 2, we all use normalized data to eliminate the distortion resulted by different units, except the last two questions. See [Still about standardization #481](https://github.com/UC-MACSS/persp-analysis/issues/481).**

```{r}
crime = read.csv("/Users/Rex/Desktop/Perspective/HW/persp-analysis/students/yu_liqiang/yu_liqiang_hw8/USArrests.csv")
head(crime)
crime_tidy <-crime
crime_tidy$State <- NULL
autoplot(prcomp(crime_tidy), data = crime, colour = 'State',label.size = 3,loadings = TRUE,loadings.label = TRUE,loadings.label.size = 3)+labs(title="PCA on USArrests")

```

###Problem 2

Perform $K$-means clustering with $K=2$. Plot the observations on the first and second principal components and color-code each state based on their cluster membership. Describe your results.\

With K-means clustering (K=2), we clusters data (normalized) for 50 states into 2 clusters. Each cluster contains approximately the same number of observations. Cluster 1, labeled by blue points, represents the cluster of states with low crime rate while Cluster 2 (organe points) represents states with high crime rate.
```{r}

autoplot(kmeans(crime_tidy, 2), data = USArrests, label = TRUE, label.size = 3)+labs(title="K-means clustering on USArrests (K=2)")

```

###Problem 3

Perform $K$-means clustering with $K=4$. Plot the observations on the first and second principal components and color-code each state based on their cluster membership. Describe your results.\

With K-means clustering (K=4), we clusters data (normalized) for 50 states into 4 clusters. Each cluster contains a different number of observations. From the left to the right, Cluster 1, labeled by blue points, contains about 9 observations and represents the cluster of states with the lowest crime rate among all 4 clusters. Cluster 2, labeled by green points, contains about 17 observations and represents the cluster of states with low-to-medium crime rate among all 4 clusters. Cluster 3, labeled by organe points, contains about 9 observations and represents the cluster of states with medium-to-high crime rate among all 4 clusters. Cluster 4, labeled by organe points, contains about 14 observations and represents the cluster of states with the highest crime rate among all 4 clusters. 
```{r}
autoplot(kmeans(crime_tidy, 4), data = USArrests, label = TRUE, label.size = 3)+labs(title="K-means clustering on USArrests (K=4)")
```

###Problem 4

Perform $K$-means clustering with $K=3$. Plot the observations on the first and second principal components and color-code each state based on their cluster membership. Describe your results.\

With K-means clustering (K=3), we clusters data (normalized) for 50 states into 3 clusters. Each cluster contains a different number of observations. Cluster 1, labeled by blue points, contains about 21 observations and indicates the cluster of states with the lowest crime rate among all 3 clusters. Cluster 2, labeled by orange points, contains about 14 observations and represents the cluster of states with medium crime rate in United States. Cluster 3, labeled by organe points, contains about 16 observations and represents the cluster of states with the highest crime rate among all states in US.
```{r}
autoplot(kmeans(crime_tidy, 3), data = USArrests, label = TRUE, label.size = 3)+labs(title="K-means clustering on USArrests (K=3)")
```

###Problem 5

Perform $K$-means clustering with $K=3$ on the first two principal components score vectors, rather than the raw data. Describe your results and compare them to the clustering results with $K=3$ based on the raw data.\

Using the score vectors instead of raw data (normalized), we get the same clustering result as in Problem 4, except the scale on x-y axis, which is reasonable, since after normalization, the score vector is linear dependent with principal components. Things would be different if we did not normalize our data.\

Ref:[What are principal component scores?](https://stats.stackexchange.com/questions/222/what-are-principal-component-scores)

```{r}
crime_pca <- prcomp(crime_tidy)
scores<-crime_pca$x #Score vectors stored in crime_pca$x
centers=as.data.frame(kmeans(crime_tidy, 3)$centers)
pca_score=as.data.frame(scores)
rownames(pca_score) = crime[,1]
autoplot(kmeans(as.data.frame(pca_score), 3), data = pca_score, label = TRUE, label.size = 3,scale = FALSE)+labs(title="K-means clustering on USArrests (K=3) on score vector")
```

###Problem 6
Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.\
See graph below.
```{r}
crime_hc<-hclust(dist(USArrests,method = "euclidean"),method="complete")
labelColors = c("#CDB380", "#036564", "#EB6841", "#EDC951","#556270", "#4ECDC4","#1B676B", "#FF6B6B", "#C44D58")
clusMember = cutree(crime_hc, 9)
# function to get color labels
colLab <- function(n) {
    if (is.leaf(n)) {
        a <- attributes(n)
        labCol <- labelColors[clusMember[which(names(clusMember) == a$label)]]
        attr(n, "nodePar") <- c(a$nodePar, lab.col = labCol)
    }
    n
}
# using dendrapply
crime_hc_dend <- as.dendrogram(crime_hc)
clusDendro = dendrapply(crime_hc_dend, colLab)
plot(clusDendro, main = "Hierarchical clustering on USArrests data", ylab="Height", type = "triangle")
```

###Problem 7
Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?\
Cutting the dendrogram into three distinct clusters we get results below. It is clear to see which states belongs to which cluster below.

```{r}
op = par(mfrow = c(2, 2))
crime_dend=as.dendrogram(crime_hc)
plot(cut(crime_dend, h = 150)$upper, main = "Upper tree of cut at h=150",type = "triangle")
plot(cut(crime_dend, h = 150)$lower[[1]], main = "First branch of lower tree with cut at h=150",type = "triangle")
plot(cut(crime_dend, h = 150)$lower[[2]], main = "Second branch of lower tree with cut at h=150",type = "triangle")
plot(cut(crime_dend, h = 150)$lower[[3]], main = "Third branch of lower tree with cut at h=150",type = "triangle")
```

###Problem 8

Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation $1$. What effect does scaling the variables have on the hierarchical clustering obtained?\
After normalizing our data, we get a totally different result. The reason is that if we do not normalize our data, variables in our data set with a large valued units will be dominant the dissimilarity and other variables with a small valued units will contribute litte to the dissimilarity.

Ref:[Reason to normalize in euclidean distance measures in hierarchical clustering](https://stats.stackexchange.com/questions/30317/reason-to-normalize-in-euclidean-distance-measures-in-hierarchical-clustering)
```{r}
crime_hc_scale<-hclust(dist(scale(USArrests),method = "euclidean"),method="complete")
labelColors = c("#CDB380", "#036564", "#EB6841", "#EDC951")
clusMember = cutree(crime_hc_scale, 4)
# function to get color labels
colLab <- function(n) {
    if (is.leaf(n)) {
        a <- attributes(n)
        labCol <- labelColors[clusMember[which(names(clusMember) == a$label)]]
        attr(n, "nodePar") <- c(a$nodePar, lab.col = labCol)
    }
    n
}
# using dendrapply
crime_hc_scale_dend <- as.dendrogram(crime_hc_scale)
clusDendro = dendrapply(crime_hc_scale_dend, colLab)
plot(clusDendro, main = "Hierarchical clustering on normalized USArrests data ", ylab="Height",type = "triangle")
```