# Final Exam

```
Class: MACS 30000
Author: Jingwen (Fiona) Fan
Instructor: Benjamin Soltoff
```

## I. The 2014 paper
### 1. Summarize  

#### Summary of Research Design
The 2014 paper is an observational study that utilizes big data with human computational components to empirically test for the differences in prices between hosts of different races. The data obtained consist of the Airbnb listings for New York, NY as of July 17, 2012. Each listing includes the price that the host is asking, the characteristics of the host, the characteristics of the apartment, host's guest reviews, various average ratings and etc. To control for the quality of the apartments, the authors employed Amazon Mechanical Turkers to rate the quality of the apartment judging from the listing's photos. They also hired a different set of Turkers to identify the ethnicity of the hosts. Having all the measures, they ran regressions to see if being black negatively affect the price the host asks for, controlling for characteristics/quality of the apartment (number of beds, whether whole apartment, various ratings), host characteristics (whether has LinkedIn, Facebook and etc), and the quality of posted pictures. Their results show that controlling for all these factors, black hosts still earn 12% less for a similar apartment. Their results aid the discussion on whether websites like Airbnb should employ platform design change to actively reduce racial discrimination.  

#### How Computational Methods Help Ask and Answer the Question 
The question that the authors set out to ask, whether there exists racial discrimination, is a historically significant one that bears contemporary significance, given the heated racial disputes still going on in this country. The authors, being the HBS elites that they are, capture racial discrimination happening in the most up-and-coming online rental platform. The damages that racial discrimination existent in online platforms like Airbnb can do are comparable to the racial discrimination happening in traditional places like workplace and schools. However, before the advance of computational social science methods, such damages have not been studied in a systemic and empirical way. Now, with the advanced web-crawling "technologies", we can harness the power of big data to generalize insights about human behavior without any intervention. This avoids any potential biases existent in self-reported measures, e.g. obtained via surveys.  

In addition to utilizing big data, the authors also employed cheap human computational labor via Amazon Mechanical Turk in two parts, one in evaluating the race of the landlords, and one in subjectively evaluating the quality of the apartment, both are integral parts of the regression. Without AWS, the such identification and evaluation would be very expensive and an overkill for Harvard undergrad research assistants. Such mass collaboration allows for massive data size (or in this case, matching data size to the big data) of subjective evaluations, under a controllable budget.  

### 2. Evaluate  

The key component of the paper is the regression that the authors ran, controlling for all the measures they obtained from big data and the human computational components. The empirical analysis design seems intuitive, and thus the quality of the design hinges upon the quality of the data and whether the authors operationalize the data in a correct fashion. Not surprisingly, I will evaluate the data based on the 10 characteristics mentioned by the up-and-coming, newly-minted social science bible: Bit by Bit.   

The dataset conforms to all three good characters of big data: big, always-on and non-reactive. First, even though the data only consist of the Airbnb listing in NY, given the racial diversity, population fluidity and pure size of the megacity, valuable conclusions can be drawn from the dataset. Whether such conclusions stand robust to other geolocations is something worth studying, and so did the 2017 paper. Secondly, the data is always-on, as long as Airbnb doesn't run out of business. In fact, the fact that it is rapidly growing (almost quadrupled its size in mere 3 years) allowed for larger and larger dataset as time moves on. Third, price is something determined by the market, and not something determined by subjects' response. This serves as the perfect proxy to study racial discrimination due to its accuracy and non-responsiveness from the respondents.  

However, the dataset is not without its flaws. First, the Airbnb data by itself is inaccessible. As the authors mentioned, ideally their analyses should include both price and demand effects. However, Airbnb is unwilling to share data like guests’ reviews of a fraction of a host’s prior transactions, leading to a missing part of the analyses. Second, as mentioned above, population in NY might not be representative of the whole US population. The 12% price difference might not hold the external validity in rural counties, or Southern major cities. Third, even though in this paper alone system drift is not a problem because the dataset is a snapshot of Airbnb listing rather than longitudinal, the guests captured in this paper could be different from the guests captured in the ensuing 2017 paper considering Airbnb more than quadrupled its size. Similarly, algorithmic confounding is also not a concern in that there is not motivation for Airbnb to promote such racial discrimination. In fact, the authors are trying to lobby platforms like Airbnb to conceal host pictures for the removal of racial discrimination. Finally, the data is somewhat dirty in that the pictures of the apartments do not directly translate into their qualities, and their ratings can be given by a self-selected group of lovers/haters. To account for this the authors used cheap Mechanical Turkers to translate the pictures into something regression-able.  

The authors checked their internal validity by restricting their dataset to only black and white hosts and the results stood robust. They also pointed out their inability to separate taste-based discrimination (users just don't want a black host) and statistical discrimination (user's inference that black hosts have inferior apartments). As for external validity, as mentioned above, even though NY is a megacity where valuable insights can be drawn, the specific 12% different might not be accurate for all states in America. For construct validity, I would give the authors an A+ for using prices as a proxy for the penalty the black people receive for being black. As mentioned above, the authors also wished to see if black hosts received less demand in general, but due to limited access to data, this did not happen.    



## II. The 2017 paper
### 1. Summarize  

#### Summary of Research Design 
The 2017 follow-up paper paints the picture of the existent racial discrimination in online marketplaces like Airbnb from the guests' angle, rather than the hosts, as was the case in the 2014 paper. To unearth the causal relationship between being black and being rejected by the Airbnb host, they designed a field experiment.  

For data collection in preparation for the experiment, like the 2014 paper, they crawled the big data from Airbnb, but this time in five cities: Baltimore, Dallas, Los Angeles, St. Louis, and Washington, DC as of July 2015, short of their original goal of 20 due to technical limitations imposed by Airbnb. They saved the information of the hosts, like picture, number of listings, listing reviews and etc. They targeted only one listing per host so that one host won't receive the treatment multiple times. As was the case with the 2014 paper, they hired Mechanical Turkers to identify the race of the hosts. They also collected information of the listings like price, number of rooms, past ratings and etc. Additionally, they recorded the geographical location of the listings to control for the neighborhood quality, by linking the geographical information to census data.  

The experimental setup is such: the authors created 20 bogus accounts, identical in all aspects except for names. The names distinctively pertain to the male/female black/white divides by a previous study. The 20 accounts are then divided into 5 African American females, 5 AA males, 5 white males and 5 white females. The validity of the names were again verified by a survey. They then sent 6400 messages to 6400 hosts inquiring about an opening that the host has 8 weeks in advance. The five accounts were randomly assigned to the 6400 messages. They then track the responses in the following 30 days. A poor and soulless research assistant then hand-label the 6400 messages into 11 categories of responses, which then got hard-coded into a “Yes” and "No" response by different measures of "Yes" and "No" responses. They then regress the response on various measures, obtained through the big data crawled from Airbnb, and measures obtained through AMT and Face++ (to be discussed in detail down).  

#### How Computational Methods Help Ask and Answer the Question 

There are several components of the authors' computational approach: the first one is the web-crawling used to obtain the big data. Like the 2014 study this allows the paper to fathom racial discrimination existent online in a empirical way, but this time with the addition of the experiment, also in a causal way. The second component, like the 2014 paper, they employed AMT labors to qualify the host profile pictures into race, gender, age and etc. The race, key feature of interest were validated across three Amazon Turkers, or manually coded if all three Turkers disagreed. The third component is the deployment of facial recognition software Face++ to also categorize pictures into age, race and gender, but on the past guests of hosts. This allows for the exploration on a host's past attitude towards African Americans and her new possibility of accepting an African American guest. The fourth component is the linkage of the coordinates of listings to the census data for evaluation of the neighborhood. This allows for the study of heterogeneity of treatment on hosts in different neighborhoods. Finally, the key computational tool that they design is the automated tool that sent out the 6400 messages and tracked the responses after 30 days for the key feature -- acceptance rate to be regressed upon. All these tools allow the researchers to answer the question of whether race plays a causal role in the host's decision to accept the guest.  

### 2. Evaluate  

Unlike to the 2014 study, which only asks the question of whether racial discrimination exists, the 2017 study tries to not only see if such existence persists, but also fathom the scope of racial discrimination by setting up a causal relationship between race and chance of rejection of the guest. This is good for evaluating the forgone revenue Airbnb might have due to racial discrimination, and thus serves as a motivation for them to reduce racial discrimination. The authors took the approach from the guests' point of view because it would be very hard and ethically dubious to set up a number of fake host accounts when the authors have no properties in those 5 cities. I will be evaluating the experiment for its validity, heterogeneity of treatment effects, and causal mechanisms.  

#### Validity
* **Statistical Validity**  
The fact that the paper got published in American Economic Journal attests to the fact that the statistical tests (namely regressions) in the paper were done correctly, or at least in compliance with most economists' taste. One thing that stuck out to me was the low $R^2$ scores across all regressions, averaging somewhere below 10%. Granted a low R^2 is normal for social science researches, but the R^2 for some of the key results, like the ones in Table 2, are lower than my expectations. One reconciliation for this could be that the regressions here are not used to be predictive, but rather to reveal the causal relationships, in which case the significance level for the results carries more weight than the R^2.  

* **Construct Validity**
Similar to the 2014 paper, the race is operationalized very well using a human-identified construct. However, I think the acceptance rate of inquiries is a worse construct than the listed price for racial discrimination. Here, the reason for rejecting the inquiry was not coded into the regression. Without having guests of different race vying for the same opening, it is hard to pinpoint that race is the only factor affecting the landlord's decision not to say Yes. Also, a lessened chance of getting a positive response from the landlord does not necessarily translate into tangible penalty African American experience for being black.  

* **Internal Validity**  
I would argue the internal validity is preserved in this study for the following reasons. First, the accounts with different names and genders were randomly assigned to the 6400 hosts. The randomness is the key to ensuring causality in this experiment. Secondly, most procedures here are automated, like sending the message to the host, or following up after 30 days. The only places where human fallacy could affect internal validity is the human computational part, and where the research assistants manually transcribe the responses into categories. However, for the former, the authors uses three people to validate the same picture, reducing chances of error. And for the latter, the authors found that results stood robust applying different definitions of "Yes" and "No".  

* **External Validity**
In this study, since racial discrimination is operationalized as the acceptance rate of inquiring messages about a vacancy, the results might be restricted to online platforms similar to Airbnb like [Trippin.com](www.tripping.com). Also, the authors fell short of their desired sample size. They only sampled 5 cities and sent 6400 messages, instead of their original planned 20 cities and 10000 messages due to technical restrictions imposed by Amazon, to preserve their experimental integrity. The 10000 number was calculated from the 2014 study to preserve a similar effect size. Failing to do so could jeopardize the study's external validity. 5 cities also seem a bit far from their originally planned 20 cities for preserve geographical external validity. The results from this study of course bear external validity, but to as much as the authors have expected.  

#### Heterogeneity of Treatment Effects
The authors did an exceptional job in detailing the heterogeneity of treatment effects. They examined the results across different subgroups, divided by host characteristics (race, gender, pictures, reviews, past guests), listing characteristics (location, price), and even by the specific experimented names. They found that heterogeneity of treatment effects exists in breakdown groups by specific names, and past guest history (whether contains African American). They found that the race gap drops sharply among hosts with at least one recent review from an African American guest.  

#### Causal Mechanisms
In cross-tabing the regression across many groups of variables, the authors touched on the causal mechanisms. They found that their approximately 8% gap in positive response rate stands robust to many variables. In other words, factors like race of the host, location of listing are not the intermediating factors causing the gap. However, as mentioned above, past guest history (whether contains African American), although sounding less like a mediating factor, but rather a predictor, affects the acceptance rate. Also, just like in the 2014 paper, the authors cannot tell the difference between distaste and statistical discrimination. Thus, the authors touch on the causal mechanism of racial discrimination, but did not reach a telling conclusion. As the authors so elegantly put it, the findings they had were "in tension with pure taste-based discrimination", but they also found "some evidence against pure statistical discrimination", so there must be "a more nuanced story than either of the classic models".  

Another note: there should also be ethical concerns towards the design of the experiment, even though the study is IRB approved. The 6400 hosts should receive a note explaining the design the purpose of the experiment ex post.  

## Linking them together 
* Identify the value-added of conducting both research projects. That is, what do we learn from running both an observational study and a field experiment that we could not learn from just one of these methods?  

To me, the 2014 paper seems like a precursor to the 2017 study. The results from 2014 acknowledge the existence of racial discrimination from the perspective of landlords (discrimination towards them). Normally, observational studies win over experimental ones with their robust external validity. Here it's somewhat the opposite case. With data confined to New York City, the external validity of 2014 study is not sufficient. Its internal validity is more preserved in that it controls for observable attributes of each listing with its exhaustive tables of regressions. Still it does not offer enough internal or external validity to paint a thorough picture of how racial discrimination exists. However, their 2014 findings were sufficient for them to apply for more fundings for an experiment of larger scale that can causally explain the racial discrimination, and there yielded the 2017 paper.  

The 2017 study is larger in scale than the 2014 one, in addition to offering some causal insights. The study collects data from 5 cities instead of 1, and its experimental setup exclude noises on the potential underlying reasons why the hosts reject guests' inquiries. It 2017 study wins with its thorough discussion of heterogeneity of treatment effects. In fact, in its discussion of heterogeneity of treatment effects (Part D), it incorporates observational patterns of previous guest profiles of each host, yielding insightful result that hosts with previous positive reviews from African Americans give significantly less racial discrimination, attesting to the external validity (in authors own words) of the 2017 study (as in the experimental design does not affect the discernment of racial discrimination). However, as discussed above, both are insufficient in their coverage of causal mechanism, being unable to tell the difference between distaste and statistical discrimination (or attesting to a new theory).  

Obviously the two studies approaches racial discrimination from two angles: the 2014 one probes whether the landlords are discriminated against from their guests, penalized by a reduced market value of their listings. The 2017 one studies whether guests are discriminated against from their landlords with a lessened chance of getting a positive response for a vacancy. As discussed above, it is possible that the second experiment only switched angle to make the experiment feasible (instead of purchasing or posting bogusly about properties across US). However, the two researches combined do offer a more comprehensive view on how racial discrimination exist on online platforms like Airbnb.  

In short, I believe the 2014 study is a precursor to the 2017 study, which is more comprehensive and causal in design, and lay the theoretical and design-wise foundation for it (and provide grounds for applying for funding). It also paints the racial discrimination existent on Airbnb picture from a different angle (racial discrimination against hosts instead of guests), which enriches the authors' case.  

* Consider how you could apply a digital survey-based research design to the primary question of interest from these two papers. What are the potential drawbacks to a survey approach? How might you overcome these drawbacks?  

Here, I believe survey alone in studying existent racial bias on online platforms is not the optimal form of research in that it would incur self-reported bias, the biggest drawback of survey data. Namely, if you ask a participant whether he would be more likely to accept a request from a white guest than a black guest, the participant would see through the surveyor's intentions with ease and answer "I see what you are doing there. Uh, no". However, surveys do carry the virtue of being able to fathom people's internal thoughts. This bears significance in our study in that here we are trying to tell distaste discrimination from statistical discrimination as the causal mechanism, which ties closely to people's internal thoughts that cannot be easily operationalized as the market price or acceptance rate of inquiries.  

If I were to redesign the study to be survey-centered, I would post advertisement with the linkage to the survey (via Quadrics) across multiple platforms for housing rental services for enlarged external validity. Monetary rewards would be offered upon the completion of surveys.  In instituting the surveys, the representation error will be small as in the browsers of such online platforms usually have an agenda of using their services, which match to our target population. To avoid measurement error, the most salient of which the self-reported bias as described above, the wording and the design of the survey needs some dedication.  

As a screening procedure, the participants would first be asked whether they have properties listed on Airbnb. If they answered yes, they would be directed to a different set of question form the people who answered no. In both sets of questions, the participants would be asked if they are willing to voluntarily give their username, or some other handle that can map them to the web-crawlable data, which will be used to link digital trace data (like the big data employed in the 2014 and 2017 studies) to the survey data. In doing so we can shy away from potential ethical concerns. The guest participants would first be asked to rank a series of factors that might contribute to their decision to stay with the guest, including price, location, race of host, ratings and etc. Then they will be asked to compare between a host of two listings with similar everything except for the variable of interest (all of the aforementioned variables, including race). In an effort to account for the drawback of self-reported bias, variables like race will be sugarcoated with smiling faces of people of different races. Finally, the participants will be asked an explicit question about if race is a significant contributor to their decision, and range of questions on whether they think factors like lower-quality apartments, higher-crime-rate neighborhoods etc. are associated with the fact that the host is black. By doing this we can gain some insights on the difference between distaste discrimination (if variable race is significant contributor) and statistical discrimination (if all other variables are significant contributors). The questions targeted towards Airbnb landlords will be similarly designed. Racial discrimination will be operationalized as how likely they would accept a guest.  

Another way of addressing self-reported bias is to compare their self-reported scores with the actual operationalized outcomes in the two studies. As an instance of enriched asking, the survey data will then be linked to the web-crawled big data via their offered username (or some other mapping measures). Their reported measures and observed measures will be compared. Regressions similar to the ones in the papers will be run to see if their self-reported measurement of bias contributes to the actual prices and percentage gap between black/white tenant/landlords.  

Another potential concern could the low-response rate. To address the problem, the authors could up the monetary reward, or collaborate with those online platforms (whose interest might be aligned to fathom the lost revenues due to racial discrimination) to offer some kind of voucher/rebate for participation in the survey.  


