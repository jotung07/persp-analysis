Tom Curran
Final Exam
MACS 30000
December 6,2017

### Digital Discrimination: The case of airbnb.com

### (3 pts) Summarize the research design and explain how the research design leverages computational methods to ask and answer a question.

Edelman and Luca’s paper Digital Discrimination is an exploration of the influence of racial bias on price on the supplier side of a market place. Using Airbnb.com as a case study, Edelman and Luca ask whether the race of the landlord (person advertising the rentable space) leads to charging lower prices due to the bias of the renter/guest. In other words, do minorities have to charge lower prices, controlling for similar observable features, for the same type of unit as non-minority landlords. According to the author’s analysis, non-black hosts charge approximately 12% more than their black counterparts, holding factors like rating, location and other features constant. Edelman and Luca’s paper exhibits several properties of computational methods learned in class. The paper takes advantage of computational methods by combining aspects of an big data, observational studies and surveys to delve into their research question. 

To answer the research question, Edelman and Luca leverage computational methods of digital data. To understand the impact of racial bias, the authors aggregate individual landlord’s profiles in New York City. The profiles, as posited by the paper, are mechanisms to establish a connection between the landlord and the renter to establish trust, which is an increasingly important factor in the ‘gig’ economy in which Airbnb.com participates. The profiles contain different pieces of demographic information about the landlords, most importantly photos of the landlord that are used to identify race. Other pieces of information that are collected from the website are the location of the rental, the price per night, and photos of the rental. In the analysis, to isolate the impact of bias, Edelman and Luca control for various aspects that could influence price (table 2). Variables and influences on price that are controlled for are the number of rooms, different user generated ratings, social media presence and the quality of pictures in the profile and unit for rent. After collecting the data, the authors use Amazon Mechanical Turk to code the races of the landlords, as well as rate the photos of the rented spaces for each landlord. The survey deployed on Amazon Mechanical Turk was an attempt to convert a qualitative observation to a quantitative observation. After controlling for these factors, Edelman and Luca found that non-black landlords were able to charge 12% higher prices for similar rentals. 

The Edelman and Luca paper takes advantage of the benefits of big data, specifically, that they are able to collect large sample sizes, the data is real-time and that it is non-reactive. As with any statistical analysis it is important to have a sufficiently large sample size. Since Airbnb.com is one of the most popular sites for renting properties and the fact that they used New York as a sample satisfies the sample size requirement for statistical analysis, a explicit benefit of the big data in computational social science. Second, using real-time data, the analysis stays relevant and can capture current attitudes if modeled correctly. Finally, the analysis utilizes the non-reactive component to big data, meaning that the source of measurement is not likely to change. This proves especially useful for Edelman and Luca’s goals since what they are measuring is culturally sensitive. From a societal perspective, people do not want to be perceived as racist, so they will try to avoid the appearance of having any type of racial bias. This comes into play when attempting to measure or understand bias since people will avoid the appearance of bias it would lead to false data in any sort of experiment, this is more commonly known as the Hawthorne effect. Edelman and Luca can leverage the benefits of big data to answer their question of racial bias and landlord prices. 
While the researchers leverage the benefits of big data, they also run into several issues that are inherent to digitally sourced data. First, the researchers weren’t allowed to access any of the data on the renters themselves. The renter data is both sensitive and potentially algorithmically confounded. It is sensitive because it is transactional information and opening that type of data to researchers is a security risk to the individuals using Airbnb. The data collected can also be algorithmically confounded because, like most e-commerce websites, Airbnb could use a recommendation engine that gives certain landlord an unseen advantage over others which would then influence the landlord’s characteristics that Edelman and Luca try and control for.  

Beyond leveraging aspects of big data in their research, Edelman and Luca’s analysis utilizes observational methods, which components of survey methods as well. Using Salganik’s framework in Bit by Bit, we can see that the paper utilizes the several internal uses of observational studies and big data. First, the study “counts things” using the data set collected from Airbnb landlord profiles. Separating the landlords into “non-black” and “black” landlords, the paper counts the number of units by price per night, as well as observing the distribution of all prices (not broken up by demographic).

Perhaps most importantly, the methods that Edelman and Luca employ (i.e. using big data and observational studies) allow them to approximate an experiment. If this paper’s research question were to be tested via actual experiment it would prove to be extremely expensive, unethical, and a logistical nightmare. However, with the advent of big data and digital data sources those challenges can be circumvented. Furthermore, as mentioned earlier, utilizing computational methods like big data avoid confounding factors like the Hawthorne effect that can potentially yield inaccurate data. Here, the researchers attempted to control for almost every other factor in a Airbnb landlord’s profile in order to create pairs of landlords where race is the only difference between the two. This is similar to the ebay golf club experiment that Salganik outlines in his chapter on matching.

The Airbnb study takes advantage of big data and natural (observational) experiments in that the participants are random on both the landlord and renter side of the market (i.e. supply and demand). Since anyone can sign up to rent their space, and the only selection criteria that the researchers used is that they live in New York City, there is little to no risk for selecting people for the experiment based on observable or unobservable characteristics. Selecting on either (or both) observable or unobservable characteristics leads to bias. 

### (4 pts each) Evaluate the effectiveness of each paper's research design independently. That is to say, what do we learn from each paper on its own? What are the limitations of each paper on its own? Think back to Salganik's characteristics of big data and our assessment of experiments' validity, heterogeneity of treatment effects, and causal mechanisms. Draw on these methods of assessment as you evaluate the effectiveness of each paper.

The conclusion that observational study on Airbnb.com rental prices and racial bias yielded was the non-black landlords are able to charge 12% more per night for their rental than black land lords. The merits of the observational study are that they effectively employ the merits of big data to answer their research question. 

Specifically, as mentioned previously, the study is able to get a large sample size that is as if randomly assigned. Here, the random assignment is race and the outcome is the price per night for their rental unit. Furthermore, the data that the researchers collected is non-reactive and always on. In other words, people using the platform can’t necessarily change their behavior and the platform is constantly updating and collecting information in real time. The paper also found a way to address issues of incomplete data. A drawback to big data approaches is that data can be incomplete, and a key piece of the “incompleteness” is that we cannot know how people behave on other platforms. Thankfully, Edelman and Luca found a way around this obstacle in taking the presence of landlords on other social media platforms into their analysis. Other key pieces of ‘incompleteness’ were solved through employing amazon mechanical Turk to classify the demographics of landlords using the Airbnb.com profile picture. Using these two digitally enhanced computational methods (amazon mturk and other social networks) fills in a pitfall of big data that allows for a more thorough analysis. 

Though this paper has many merits, it also falls short in several aspects. Chief among these short falls is identifying race as the driving factor in discrepancy of rental unit prices. Because this is an observational study, it is very difficult to identify causal mechanisms. One of the biggest down falls to creating a causal link between race and rental unit price is the customer, the renter actually getting the unit. While Edelman and Luca attempt to justify their model without the consumer by controlling for things like number of bedrooms, rating and location it still leaves room for correlation with error terms, the error term containing the observed and unobserved characteristics of the consumer. If anything, the paper identifies that there is some relationship between race and rental price, but it jumps the gun in trying to tie direct causation to race. 

Salganik identifies four kids of validity for experiments in computational social science research; internal, external, statistical, and construct validity. The paper is internally, statistically and constructional valid. However, the paper lacks external validity. This lack of external validity is again primarily due to the fact that the consumer side of the market is not incorporated at all into the analysis. For one thing, New York City has one of the highest costs of living in the country and that is bound to influence the price per night that a landlord charges. Because the scale of cost of living is different across the country, it threatens the external validity of the observational study. Second reason that the observational study has threats to external validity is because the consumers purchasing the rooms for rent are have different reasons for visiting New York City. The reason for visiting could heavily favor one part of the city over another thereby increasing demand for the rental unit which is a determinant of price. This can be an external threat to validity because certain parts of the country may be not be as tourist focused as new York city

***

## Racial Discrimination in the Sharing Economy: Evidence from a field experiment

### (3 pts) Summarize the research design and explain how the research design leverages computational methods to ask and answer a question.

In Edelman, Luca and Svirsky’s paper Racial Discrimination in the Sharing Economy: Evidence from a Field Experiment explores the influence of racial bias in the sharing economy model. Specifically, the researchers conduct a field experiment with Airbnb.com to test if a guest’s race influences the rate of acceptance to use a host’s rental property. As a result of their experiment, Edelman, Luca and Svirksy saw that African American profiles were 16% less likely to be accepted for a rental unit compared to their white counter parts.

Edelman, Luca and Svirsky created a randomized control experiment in order to investigate the role of racial discrimination with Airbnb.com. First, the researchers went through a rigorous data collection process that utilized several aspects of big data. Their data collection set a goal of collecting data of Airbnb rentals in the top 20 largest cities in the U.S., however, they were only able to obtain Washington, DC, St. Louis, Dallas, Los Angeles, and Baltimore. They were restricted to those 5 cities due to Airbnb’s security measures in detecting automated web scraping of data. In addition to collecting data about the rental properties (e.g. neighborhood, price, number of bedrooms) researchers were able to collect more detailed data on the property owners. The owners, referred to as landlords, had profiles that included demographic, pricing and social information as well as photos of the landlords. The data set was extended even further when the researchers scraped the existing reviews of the rental property and the photos of the renters than came with said reviews. The experiment also employed the used of Amazon Mechanical Turk to code the races of various landlords. 

The data collection process provided a robust data set that allowed the researchers to carry out their experiment. To test their research question, the researchers made four different of fake users: African American Male, African American Female, White Male and White Female. Fake accounts were created for each group and each account given either a distinctly African American name or a distinctly white name (names were based on previous research to be deemed either “white” or “black”). From there, emails were sent out to the property owners from the collected data set and the type of response that each account received after their inquiry was recorded. 

From the responses, the researchers analyzed and sliced the data between various categories and variables to investigate their research question. They found that across a wide range of variables and functional forms, the African-American accounts were discriminated against at the same rate. That is to say the African-American names had more ‘no’ responses to their inquiries no matter things like landlord gender, race, property neighborhood, number of bedrooms and other variables the researchers collected.

Edelman, Luca and Svirsky leverage several aspects of big data and digitization of social experiments. The design of the experiment employs a randomized control trial, which is a traditional research framework in social sciences, but by employing digital techniques, the researchers were able to extend their efforts much further and with greater detail. In Bit by Bit, Salganik refers to the Analog-Digital Lab-Field diagram to plot the dimensionality of experiments in the digital age. With Edelman, Luca and Svirksy’s paper we can safely say that the experiment was on the far end of both digital and field ([here](http://www.bitbybitbook.com/en/running-experiments/lab-field/)). A field experiment has several distinct advantages over a lab experiment. First, a field experiment more is a more natural representation of the real world. With lab experiments, researchers control the environment and that can sometimes be a poor reflection of the conditions an event would take place in the real world. For example, setting strict lab controls in psychology allows researchers to evoke a specific response, but the strict controls often do not reflect reality. With field experiments, combined with the strength of Randomized Control Trials experimental design, experiments are more representative of groups of participants, simulate real world situations and avoid things like the Hawthorne effect.

The second dimension to Salganik’s framework is the Analog-Digital dimension. Under Salganik’s framework we can consider the experiment to be “fully-digital”. Fully-digital experiments are “experiments that make use of digital infrastructure to recruit participants, randomize, deliver treatments and measure outcomes” ([Salganik 4.3](http://www.bitbybitbook.com/en/running-experiments/lab-field/)) . This dimension is evident in the research design since the entire experiment takes place through digital channels. 

Being a fully-digital experiments means that the researchers leverage the benefits of big-data to help answer their research question. The experiment leverage three benefits of big data, benefits that Salganik defines as being “big”, “always on” and “non-reactive” ([Salganik 2.3.1](http://www.bitbybitbook.com/en/observing-behavior/characteristics/good/)). First, the researchers were able to create web scarpers, that is to say automate data collection from websites programmatically and not manually. This has the benefit of being much faster, avoids human error, and leverages the popularity of a Airbnb to make the data set robust and “big”, collecting thousands of properties in their sample cities. Second, the experiment uses big data’s “always on” aspect. Here, Airbnb is continually collecting and updating their information which benefits researchers by being able to track minute changes as well as have the most up to date information. Finally, the benefit of being “non-reactive” means that the subjects being tested do not know that they’re being experimented on and therefore cannot change their behavior based on who is observing them. For example, since this experiment deals with a socially sensitive subject matter like racism, if the experiment was conducting in a lab people may be inclined to change their behavior because they do not want to be perceived as racist or biased. This is more commonly known as the Hawthorne effect. 

There are drawbacks to big data that the experiment is potentially subject to, using again Salganik’s framework of big data, are drift, being algorithmically confounded, containing sensitive data, incomplete and inaccessible. First, the risk of drift is present because the audience of the Airbnb platform could change. Changing audiences could influence the landlord side of the market, and if the experiment’s accounts are distinct from the typical user it could give inaccurate results. Second, like most e-commerce sites, Airbnb most likely uses algorithms to determine like and dislikes as well as potential matches. Given this, it could be that some aspect of the data collection was confounded by a algorithm used by Airbnb. Third, the data set contains sensitive data in that it has detailed personal information of landlords and that if information was to be released about landlords that were biased it could breach ethical protocol. Fourth, the profiels that were scraped may not be complete. While it would benefit landlords to have a complete profile, it does not guarantee that they will. Missing information can often lead to inaccurate samples and analysis. Finally, as seen during the data collection phase, Airbnb can make the data inaccessible to digital data collection by using different security measures.

### 4.Evaluate the effectiveness of each paper's research design independently. That is to say, what do we learn from each paper on its own? What are the limitations of each paper on its own? Think back to Salganik's characteristics of big data and our assessment of experiments' validity, heterogeneity of treatment effects, and causal mechanisms. Draw on these methods of assessment as you evaluate the effectiveness of each paper.

Ultimately, the paper concluded, based on their model, that African Americans were 16% more likely to not be accepted by a landlord to rent a property. The researchers successfully combine the uses of big data and experimental design to investigate their research question.

In regards to the uses of big data, the experiment successfully leverages the popularity of Airbnb in order to satisfy the requirement of experiments to contain a large enough data set. This is critical because a smaller data set could potentially yield potentially statistically biased results. The size of the data set would have influence the “completeness” of the data set as well. Salganik suggests that a pitfall of digitally sourced data is that it is incomplete. However, the researchers were able to collect enough data that even if profile were incomplete, they could discard them from the sample. They were also fortunate because there are natural economic incentives to landlords having a complete profile as it is better for business. Based on Salganik’s big data framework, the experiment is most at risk by having sensitive, inaccessible data and being algorithmically confounded. 

The sensitivity of the data comes from the fact that the hosts of Airbnb properties are real people. The collected data contains sensitive information regarding names, addresses and demographics. This data set, while public, could potentially be a security risk if made available to the public as it could lead to violations of privacy. Second, because these are real people releasing information that they are racially biased could be damaging, especially because several of the tested landlords had built entire businesses on Airbnb’s platform. The data is also inaccessible to the researchers because Airbnb has the right to protect their information from being collected using things like web scrapers. The data collected, hosted and maintained by Airbnb is their property and they can cut off access to said data at will, meaning that if the research could damage the business the website has the right to cut off researchers from the critical source. Since the researchers are not using another website, this could potentially be a massive issue in the future if the company changes its security measures to prevent web scraping.

The algorithmic confounding in the experiment takes place in two places. First, Airbnb is almost definitely using algorithms to provide matching properties based on preferences for the average user. It stands to reason that if they are using them on the consumer side of things, they are most definitely using them on the demand side of the market. This complicates matters as we are unable to see how those algorithms influence how and when things like communications between consumers and landlords are influenced. Second, the research team uses an algorithm to detect the races of prior tenants. This could further complicate or bias the research if there is some unseen unintended consequence within the algorithm.

Because this is a field experiment, it must hold up to concepts of validity, heterogeneity of treatment effects and causal mechanisms. Salganik provides four understandings for validity: internal, external, statistical, and construct validity. The latter two types of validity can be assumed since the researchers are well trained in statistics and they use the common understanding of racial bias in the U.S. The paper goes through great care to make sure that the results are internally and externally valid. 

When referring to internal validity, Salganik defines it as “whether the experimental procedures were performed correctly”. In evaluating this experiment for internal validity, I did not find flaws with the way the experimental procedure was conducted. The researchers able to effectively randomize their sample of landlords because the collection and dispersing of treatment was done automatically, assuming there was no inherent bias within the scraping procedure. The researchers even went as far as quality controlling the coding of landlord’s race in Amazon Mechanical Turk by having several users check for the race and if a dispute occurred a third party was brought in. The researchers also check for external validity several ways. First, by selecting more than 1 city in their experiment, they are controlling for geography. If the paper had only chosen 1 city to examine, the external validity may have been lost since cultures and attitude often change with geography. Furthermore, the results of the experiment are reproducible. They process could be deployed again on Airbnb or another website. The only threats to external validity is that the five cities where data was collected were major metropolitan areas, and did not extend their data to all regions of the country (e.g. Chicago in Northern Midwest or Seattle in the Pacific Northwest). This of course was not due to lack of insight but rather a restriction of resources. Secondly, I think the experiment could have been extended to website similar to Airbnb to solve for things like algorithmically confounding circumstances as well as increase the external validity, or generalizability, of the experiment. 

The advent of big data allows researchers to gain insights for heterogeneous treatment impacts. In other words, instead of having a general or “average” of a treatment effect, digitally sourced data and experiments allow for researchers to see how the treatment impacts different groups. Here, Edelman, Luca and Svirsky show treatment effects for multiple groups. In the paper, the authors slice the data in various ways to show the impact of different groups. For example, the paper tests the racial bias on different genders, races, on the type of rental property (i.e. whole apartment or room) and in doing so were able to answer their research question with greater detail. 

Though the researchers have a robust data set and sound methods and validity, the experiment is unable to define the mechanism that triggers racial discrimination. In otherwords, the experiment delineates a relationship between race and acceptance rate, but it does not identify the underlying reasons for why that relationship exists. As the paper says, “Our findings cannot identify whether the discrimination is based on race, socioeconomic status, or a combination of these two. That said, we note that discrimination disappears among hosts who have previously accepted African American guests” (17). This is not the fault of a poorly conducted experiment, but rather it is simply a limitation to experiments themselves. While big data has allowed for larger sample size and ability reinforce external and internal validity, it has made uncovering causal mechanism of nuanced subjects like racial bias fuzzy.

Another imporvement or change I would make to the experiment is increase the number of accounts for all four types. Rather than having 20 total accounts, five for each race and gender combination, I would increase that number. This is because increasing the number of fake accounts, and therefore responses, could increase the variation of response. Becuase there is more potential variation in the responses, we can more closely refect that reality of the situation and uncover causal mechanisms. 

***

### (3 pts) Identify the value-added of conducting both research projects. That is, what do we learn from running both an observational study and a field experiment that we could not learn from just one of these methods?
Independently, the observational study and the experiment provide insights but also have several drawbacks to the validity of their findings. However, when combined as a series, the observational study informs the experiment and vice versa. 

Observational studies can at as a gateway to a more targeted experiment and research question. Alone, the observational study faces many threats to external validity (i.e. generalizability) because they typically only look at one sample an observe a phenomenon within that sample. However, when combined with an experiment, the observation is extrapolated and its generalizability is improved. Essentially, an observational study only allows us to claim the potential presence of something or an association, but not causation. When combined with an experiment, the observational study is able to get closer to causation.

An experiment on its own may come closer to a causal explanation of an event, but runs the risk of not being similar to real world conditions. In a laboratory experiment, researchers are able to create perfect conditions to isolate and understand causation. However, in the real world where the subjects of social science exist, conditions are never perfect and the findings in a lab might not be useful and replicable. 

From running both an observational study and experiment we gain enhance our understanding and answer to the proposed research question. In the two research papers, the researchers find that non-black landlords can charge more per night for their unit and that non-black renters are more likely to be accepted by landlords. There is a clear hierarchy when it comes to combining observational studies and experiments. The first paper was written in January of 2014, and the experiment paper was published in 2017. The timeline is important here because we can infer that the experiment and its research question was generated from the observational study. Given the relationship between the two, it shows the power and value added of combining both. By using the observational study first, we are able to get a better research question and able to inform data collection efforts. 

Combining observational studies with experiments allows for researchers to provide fixes for the other’s weakness. The experiment’s findings are able help the observational study’s inability to be generalizable and the observational study’s real-world applications and findings can help inform how the experiment is construct. Furthermore, combining the two helps deepen our understanding of causation. While understanding causation can be a natural product of an experiment, by combining it with an observational study, the causation is rooted in real world circumstances. Furthermore, as in the Edelman, Luca and Svirsky’s experiment, combining the observational and experimental studies can suggest more effective policy changes when it comes to studying social issues like racial bias. Again, because observational studies only show an association and not causation, it would be hard to develop effective and meaningful policy interventions. If using just experiments alone for policy interventions, the solutions may be sound in theory but not rooted in practicality or reality. By combining these two forms of studies we can both show (theoretically) causation and suggest meaningful and practical interventions to solve issues. Together, the two forms of research and analysis provide a power tool in which to study problem or phenomenon under real world conditions. 


### (3 pts) Consider how you could apply a digital survey-based research design to the primary question of interest from these two papers. What are the potential drawbacks to a survey approach? How might you overcome these drawbacks?

One of the most powerful survey tools that digital channels of research and computational methods in social science has generated are the Ecological momentary assessment surveys. An Ecological momentary assessment, or EMA, as defined by Salganik, “involves traditional surveys, chopping them up into pieces and sprinkling them into the lives of participants. Thus, survey questions can be asked at an appropriate time and place rather than in a long interview weeks after the event has occurred”. In other words, unlike typical social science surveys like the General Social Survey, the EMA methodology captures events and the respondents action as close to the event as possible and then continue to engage the respondents over time.

The four principle components of an EMA are: “1) collection of data in real-world environments, 2) assessments that focus on individuals current or very recent states or behaviors, 3) assessments that may be event based, time based or randomly prompted, 4) completion of multiple assessments over time” ([Salganik 3.5.1]( http://www.bitbybitbook.com/en/asking-questions/how/ema/)).

If I were implementing the EMA for researching racial bias in Airbnb, for the first stage I would use the Airbnb as the “real world environment”. Obviously, the Airbnb platform is digital but it still stands as an accurate channel to capture information in a meaningful way that is reflective of real world circumstances. For the second stage, I would send an email to all those hosts who responded to the initial inquiry from the account, and intentionally leaving out those hosts that did not respond to the inquiry at all, soon after the response was received by the account. The survey would ask several questions about why the host responded in the way that they did. The survey itself would be time based, where in there was a limited time that the respondent could answer their questions. The questions would be free responses and not pre-determined multiple-choice answers. The questions would be time based in order to make the survey efficient but also capture the true sentiment of the action. If people are given limited time to respond then it should prompt them to give the first answer that comes time mind. It may not be explicit in identifying race as their primary reason for responding the manner they did, but it can reveal hidden or implicit bias. In other words, because people want to avoid the appearance of racism or bias, they will respond in a way that shows that if given a pre-set answer, but free response and immediate answer can reveal bias that they respondent may not know they have or are exhibiting. Because an important part of the EMA is assessments/surveys over time, I would continue sending emails to the hosts asking for reasons for their responses for the different accounts from which the reservation made. Furthermore, I would used an amplified asking strategy since the response rate to these types of surveys tends to be rather low.

One of the most significant drawbacks to this EMA design is the response rate. Because  the response would be sought via a survey in an email it is easy to ignore and thus getting people to answer it would be difficult. Other drawbacks to the design are that by having a free-response design it would be difficult to categorize responses into discrete reasons like the ones provided if the survey was done in a multiple-choice style survey. Furthermore, if the host has multiple inquiries a day it would be hard for them to remember which guest is which and ultimately the reason why they responded the way they did. This is further complicated if the email is not sent near the time of the event, which could be a challenge if survey deployment was not automated. Finally, the survey does start pushing ethics of survey by not allowing the respondent/host to be opt out of the survey and or experiment. 

There are several ways to overcome these challenges. First, to conduct the survey in a meaningful way, the researcher could partner with Airbnb to conduct the survey. Establishing a partnership could yield several solutions to the digital channel survey. Using Airbnb’s approval and internal platform could resolve latency issues between the event/response to the time of receiving the survey because using the Airbnb platform we could avoid sending emails and rather do popup surveys when the response to the guest is sent. Furthermore, working with Airbnb is a way to add legitimacy to the survey itself as well as a way of avoiding potential ethical issues with hosts since the survey is coming from Airbnb and not a third party monitoring their behavior. Finally, to over come the challenge of a free-response survey, I would employ text mining methods to extract sentiment and key words that could be then added to existing data sets to provide external validity as well as further develop causal relationships.

