POCA Final Exam
================
Nora Nickels
12/6/2017

Final Exam
----------

### Perspectives of Computational Analysis - Fall 2017

### Nora Nickels

Evaluating Research Designs
===========================

**Summarize the research design and explain how the research design leverages computational methods to ask and answer a question.**

Edelman and Luca’s 2014 study investigated digital discrimination in the online rental marketplace of Airbnb.com by running a digital observation study that tested for racial discrimination against landlords. To do so, the authors created a digital data set using New York City landlords on Airbnb that included information of profile photos, rental prices, and characteristics about their rental properties. They used this digital information to identify the race of the hosts and to estimate differences in prices between hosts of varying races. They found that a gap in rents listed by non-black and black hosts exists and persist even when controlling for location, reviews, and photos. They also used this digital data set information to statistically confirm the factors that determine prices on Airbnb, and utilized this information to present the main results of the research question, “Do black hosts earn less on Airbnb?”, by adding in multiple variables to control for the validity of the main relationship between host race and rental prices.

Edelman and Luca’s 2014 study leverages computational methods to ask and answer their question in a variety of ways. First of all, using information from a digital rental marketplace allows for collection of “big” data; with Airbnb having over 300,000 listings as of 2013, the authors use of pictures of all NYC landlords on Airbnb, in addition to the information about the characteristics, quality, and pricing of their properties, allows for *lots* of information on *lots* of people. Their large digital data set is good for more accurate estimates, particularly when running models that control for numerous characteristics associated with the qualities of the rentals, and good for making causal estimates from observational data. Further, the data collected from Airbnb is “always on”, as the authors can use past reviews in their analyses as well, creating further variables to control for factors such as experience as a landlord (i.e. being an avid vs. causal Airbnb host). Finally, the Airbnb platform is non-reactive; as this is a digital observation study, hosts are not aware they are being measured, and express comfort and natural behavior in a digital environment. These aspects of digital data allow for a digital observational study with solid external validity when asking the question of how discrimination exists in a digital rental marketplace, and the bigness of the data allows for answering that question statistically in a more meaningful way.

Edelman, Luca, & Svirsky’s 2017 study also investigated digital racial discrimination on Airbnb, but instead did so using a digital field experiment instead of an observational study. Also unlike the 2014 study, this study found discrimination against guests, as opposed to discrimination against hosts. The authors created twenty guest accounts that were identical aside from name. Two sets of names were used that were either distinctively African American or distinctively white. The authors measured positive vs. negative response to guest inquiries about rental availabilities in order to determine discrimination against guests, and found that African American guests received a positive response at an 8 percentage point penalty when compared to positive responses received by white guests. They also combined the data from this digital field experiment with observation data to find that effects persist for both African American and white hosts, both male and female hosts, both experienced vs. less experienced hosts, among several other differences. They used digital reviews to cross-validate experimental findings on whether the host recently had an African American guest and found that discrimination is concentrated among a subset of hosts who had not hosted African American guests. Finally, they were able to use their digital experimental results in combination with observational data of rental availability status to find that a host incurs a monetary cost in foregoing revenue by rejecting an African American guest.

Similar to the 2014 study, there are characteristics of a digital study medium that lead to advantages in terms of the questions being asked. For example, similar to the observational study, the authors leverage the use of the “bigness” of digital data, collecting data from top metropolitan areas and ending with inquiries toward roughly 6,400 listings on Airbnb across five cities (before Airbnb shut down the guest accounts). Although this did not get them to their data collection goal determined by their power analysis, this is still a large data set that collects a large amount of information on many listings. Further, looking at previous reviews and taking advantage of a data set that “always on”, longitudinally speaking, allowed for the authors to view subsets of the data to answer the question of discrimination being concentrated among a subset of hosts that have not recently hosted African American guests. Finally, with the platform being non-reactive, the external validity of hosts responding positively or negatively to guess inquiries is sound; however, the experimental design as opposed to the observational design leads to higher risks regarding this, which will be discussed below.

**Evaluate the effectiveness of each paper's research design independently. That is to say, what do we learn from each paper on its own? What are the limitations of each paper on its own? Think back to Salganik's characteristics of big data and our assessment of experiments' validity, heterogeneity of treatment effects, and causal mechanisms. Draw on these methods of assessment as you evaluate the effectiveness of each paper.**

The 2014 study, on its own, tells us that, controlling for all attributes that are readily observable to a potential tenant observing listings on Airbnb, black hosts receive strikingly different rents. Although this observational study and subsequent regressions and robustness check control for all observable characteristics, the study is unable to determine the form of discrimination that exists towards hosts and is unable to answer questions of causation with sound internal validity – one of the limitations of an observational study. With the authors only controlling for the information readily observable on Airbnb, the data may be incomplete, in that demographics that may add to the rental price differences may be unavailable. The data looking at discrimination in general may also be non-representative; that is to say, does discrimination exists in a different way for Airbnb users, as compared to users of digital domains in general, as compared to the general population? Another common issue for observational studies specifically may be that the data is dirty in the sense that is does not reflect the real actions of individuals that are of interest to the researchers; this goes along with the above ideas in some ways. Finally, as an observational study, the study does not have the ability to check whether findings are based on the identity of the guests, or those subsets that may be driving the discrimination in terms of demand and pricing. For example, the observational study does not clarify user preferences for same race hosts in terms of driving the rental pricing; it cannot determine if the discrimination is driven by homophily (in-group) bias, and therefore cannot determine if guests simply prefer hosts of the same race.

With the 2017 study being an experiment as opposed to an observational study, there are several aspects that differ in terms of what the study has the ability to teach us and also in terms of how it creates limitations and issues with the effectiveness. The study on its own gives a more causal answer to the question of how discrimination affects online rental markets – not in terms of pricing, but in terms of discrimination towards guests based on host likelihood to respond positively to rental inquires. The experiment allows for better causal inference, particularly in terms of being able to randomize specific treatment effects that are sent out to a large number of rental hosts. Although the two studies share similar issues in terms of construct validity (i.e., do these online forums of digital rental properties truly measure discrimination when we cannot measure the subconscious cognition underlying hosts/renters actions?), the experimental research design allows us to use internal validity to answer the causal question of how race affects host willingness to respond positively to users. Digital experimentation helps with external validity in the sense that the authors can reach a broader subject pool (roughly 6,400 hosts), reduce cost, look at temporal characteristics of how hosts have interacted with African American guests in the past, and reduce the need for informed consent. Further, an experimental research design allows for the observance of heterogeneity of treatment effects. Unlike the observational study, this design does allow researchers to check whether the findings are based on the identity of the hosts, and to check subgroups that may be driving the discrimination. This applies both in terms of the ability to check if users who did not have recent African American guests were driving the discrimination (they were) and to check if the discrimination is driven by homophily or in-group bias (it was not, aside from female African American hosts who gave preference to female African American guests). The multiple treatment groups and use of longitudinal observational data in combination allows for these cross checks to be made to determine a variety of heterogeneity of treatment effects.

However, the experimental research design of the 2017 study does bring in some limitations and weaknesses. With an experimental manipulation comes a nonresponse bias, which the author sought to mitigate by showing that discrimination results occur because of differences in “Yes” or “No” responses, not because of nonresponses or intermediate responses. However, excluding these responses does question the validity of the results and adds a concern in general if accounts of different races might be more likely to be categorized as spam. Finally, there are some negative aspects of digital data that are shared with the observational study, such as the concern of the incompleteness of the data, as only the visible information from profiles is accessible by researchers, or the concerns of the non-representative aspect of the data. As an experimental research design alone, we also have to be concerned about the inaccessibility of the data; this was certainly an issue for the experimental version of the study, as Airbnb shut down the guest accounts after 6,400 hosts were contacted, which was short of the data collection goal as determined by the power analysis. Finally, as the experimental version of the study does directly contact user hosts with fake guest accounts, we do need to mention the limitation of the sensitive nature of the data. Airbnb users may be less likely to use or trust the service when the realize their actions are being observed for research data, especially data that looks at a sensitive topic such as discrimination, and may be upset by the lack of consent collected and the “big brother” feeling that may come with suspecting a fake account is being used to observe their actions.

**Identify the value-added of conducting both research projects. That is, what do we learn from running both an observational study and a field experiment that we could not learn from just one of these methods?**

To begin, combining research projects that are observational and experimental in nature can be thought of as in some ways being the best of both worlds. An experiment with clean causal mechanisms, while running an observational study, which both gather lots of data on that population of interest, can be seen as balancing internal validity with external validity. The less manipulation and the more observation, the more external validity that is thought to be kept, but an experimental field study allows for gain in terms of internal validity, as discussed by the benefits above, such as being able to observe heterogeneity of treatment effects. It is important to realize that it is not just a simple matter of the experimental research design allowing for additional benefits to the benefits of the observational research design. The experimental research design would not be able to control for other aspects of demands in terms of what adds to value of rentals without a statistical analysis of data gathered from the observation studies. Further, by combining the two research studies, we are also able to look at racial digital discrimination from multiple angles; the observational research design allowed the authors to look at discrimination towards hosts, whereas the experimental research design allowed the authors to look at discrimination towards guests.

**Consider how you could apply a digital survey-based research design to the primary question of interest from these two papers. What are the potential drawbacks to a survey approach? How might you overcome these drawbacks?**

The original two studies did utilize digital surveys in some ways; they use Mechanical Turk workers to rate listings, to identify race, age, and other characteristics, etc. However, applying a digital survey-based research design to the primary questions of interest may benefit the study in terms of mitigating the concerns revolving around construct validity. Using survey-based research design may allow researchers to understand fully *why*, subjectively, hosts list their rentals at specific pricings and why hosts respond positively or negatively to guests in response to an inquiry. My thoughts for applying a digital survey-based research design would be a survey given to Airbnb hosts and guests that would provide varying degrees of information regarding both inquiries to hosts and listing information to guests. The survey could be randomized wording, in the sense that hosts and guests would randomly receive surveys that interchanged hosts with names and photos of different races, and interchanged guest inquiries in the same way. We could see direct outcomes of behavior as opposed to simply observing listing prices, while still utilizing a realistic user base of hosts and guests. The largest benefits to using a digital survey, in my opinion, would be ethical in nature. Ideally, the authors could work with Airbnb to administer surveys to guests and hosts, and these surveys could be consented to by participants. If researchers could work with the digital domain in question to send out a survey to guests and hosts, issues of ethics and inaccessibility may be lessened. Further, the survey could collect more subjective information from guests and hosts, in terms of the “why” behind why they list at certain prices and why they respond positively or negatively to inquiries.

There are multiple potential drawbacks to the use of digital surveys. Although perhaps allowing for the ability to collect more subjective information about reasoning behind users decision making, survey data may lead to the scenario where individuals who are more digitally savvy may provide more information and more accurate information than users who have a more basic usage of Airbnb. With different experiences of Airbnb usage may come different experience with technology in general, leading to differences in digital survey answer quality. To rectify this, we could take a pilot poll of the data after ten percent of targeted data collection is complete, observd the collected demographics, and find out if the users who provide more information are more avid users of the site in general, so that we are not over sampling from a subset of the population who are more avid as opposed to casual users. The goal here would be to create a more accurate frame population as the sample is continually gathered, as opposed to viewing the uneven distribution of participants at the end of data collection.

Another source of error, one that may continue from the first two studies, is that involving construct validity of the survey. There may be error in how inferences are made in translating survey answers, both objective and subjective, to behavioral discrimination. For example, individuals who are more likely to judge based on characteristics like race, or who are more hesitant based on digital profile information, may be individuals who are more likely to use digital domain websites such as Airbnb. Although a digital domain is more private than making behavioral choices in front of other members of racial groups, when being asked a direct survey questions, individuals may still subconsciously mask the true meaning behind their decisions. To avoid this measurement error, we could match survey answers to past reviews and past listings to see if past longitudinal behavior holds up with their past guests and their races.
