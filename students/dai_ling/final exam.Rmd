---
title: "Ling's Final Exam Response"
author: "Ling Dai"
date: "12/06/2017"
output:
    html_document:
        keep_md: TRUE
---


####Summary of research resign (Edelman, B. G., & Luca, M. (2014).)

In *Digital discrimination: the case of Airbnb.com* (Edelman, B. G., & Luca, M. (2014)), the researchers aimed to test whether racial discrimination against landlords exists in the online rental marketplace Airbnb.com by comparing the rental prices of similar apartments offered by Black and non-Black hosts. 

To conduct the observational study, the researchers constructed a new data set that combined pictures of all New York City landlords on Airbnb with their rental prices and information about characteristics and quality of their properties. The data set consisted of a snapshot of listings contained on Airbnb for New York City as of July 17, 2012. The price the host is asking, the characteristics of the host, and the characteristics of the apartment were collected for each listing. (2014) Moreover, the researcher also recorded the number of reviews, and the average rating for each host characteristics from Airbnb’s structured rating system.

Due to the nature of this observational study, it is crucial for the researchers to accurately quantify the perceived “qualities” of the apartments. To further improve the quality control of the study, the researchers leveraged human computation, a form of mass collaboration, to rate the appearance and quality of the apartment listings by hiring workers through Amazon Mechanical Turk to rate each listing’s photos on a seven-point scale. Furthermore, human computation was also utilized to identify the race of the hosts: after downloading all public profile pictures of New York City hosts, the researchers hired another group of workers on Amazon Mechanical Turk to code the race of the hosts.

After all relevant data were collected, the researchers then performed empirical analysis to test the racial discrimination. Due to limited availability of information on consumer demand, the researchers gave up analysis of consumer demand and focused on the role of races in listing prices. Using regressions, the researchers were able to identify some determinants of prices on Airbnb, including number accommodated, locating rating, and social network presence. The regression analysis also confirmed the importance of listing photos: listings with higher photo ratings tend to have higher prices. After taking all these price determinants into consideration, the researchers were able to eliminate about half of the racial price gap in the raw data. However, a racial price gap of approximately $16 remained. Controlling for all possible factors, the researchers found that non-black hosts earn roughly 12% more for a similar apartment with similar ratings and photos relative to black hosts. (2014)

After proposing a question about racial discrimination on the Airbnb market, the researchers successfully utilized digital traces to answer the question. Moreover, they efficiently incorporated computational methods such as human computation to facilitate the research. Without the help of human computation, the researchers might not be able to include the appearance of listings as a determinant of price. In that case, the effect of racial discrimination on pricing might be falsely evaluated, undermining the credibility of the research.

####Assessment of effectiveness (Edelman, B. G., & Luca, M. (2014).)

While the research represents an interesting result, claiming that the Black in New York City receives 12% less for renting apartments of the same quality when compared to non-Black hosts, the effectiveness of the result may be compromised by a lack of external validity and some internal limitations of the study.  

First, the study has relatively strong theoretical construct validity and internal validity. The research had a clear theoretical construct that can be easily transformed into statistical tests and measurable data.  Moreover, the statistical analysis of the experiment was carried out properly, with additional testing of different models and regression parameters. Furthermore, because the procedures of the experiment were relatively straightforward, the internal validity of the experiment is unlikely to be compromised by human errors. One possible source of errors is the human computation process: it is highly possible that different grading persons may have varying standards when evaluating the appearance of an apartment. To increase the accuracy of grading, each photo should be evaluated by several different graders, and the median or mean grade should be taken as the final grade of the photo.

An apparent weakness in the external validity of this observational study is that all the sample apartment analyzed in this research were from New York City. Although using data from a single city might make it easier to analyze how location can influence apartment pricing, it severely limits the generalizability of the research results. In fact, New York City has long been notorious for its segregating housing areas. Therefore, guests looking for apartments in New York City are likely to expect the apartments offered by Black hosts to be in inferior residential areas. Given this fact, one could reasonably expect a similar experiment conducted in another city to have drastically different outcomes. 

The research also has some internal limitations, making it almost impossible to rule out the influences of all non-racial factors that may influence the pricing, a case known as omitted-variable bias (OVB). Although the researchers did their best job to include all information from Airbnb search results, it does not simply mean that all visible information were taken into considerations. Specifically, the pictures of hosts and apartments may convey very complex information that cannot be fully captured by the evaluating processed in the experimental design. For example, instead of simply perceiving the race of the host, a guest can have an expectation of the personality of the host when viewing the photos. This expectation, may or may not be associated with race, may in turn influence the decisions of the guest.

####Summary of research design (Edelman, B., Luca, M., & Svirsky, D. (2017).)

Following the observational study, the researchers also conducted a digital field experiment to investigate whether discrimination against African American guests exists on Airbnb. (Edelman, B., Luca, M., & Svirsky, D. (2017)) In the experiment, the researchers not only leveraged human computation, but also took advantage of automated processes, such as Face++ API and web scrapping.

First, the researchers collected data on all properties offered on Airbnb in Baltimore, Dallas, Los Angeles, St. Louis, and Washington, DC as of July 2015. (The researchers started with these five cities because they had varying levels of Airbnb usage and came from diverse geographic regions.) Noticeably, for hosts with multiple listings, only one listing per host was selected. Detailed characteristics of the hosts were also collected, with the aid of human computation: workers are hired from Amazon Mechanical Turk to code the race, gender and age of the hosts. Besides the basic characteristics of the hosts, the researchers collected data on previous guests of each host using an automated tool called Face++, as well as information about each listing, and follow-up information on whether hosts were ultimately able to fill openings.

After the data collection process, the hosts were randomly assigned to four main treatment groups based on the perceived races and gender of the test guest accounts. Specifically, hosts in these four treatment groups were contacted by guests with names that signaled African American males, African American females, white males, and white females, respectively. 
After assignment each host to a treatment group, the treatment was delivered using an automated process. The researchers sent out about 6,400 messages, each inquiring about availability during a specific weekend in September 2015. The replies were collected using a web scraping tool and then coded into six categories: “No response”; “No or listing is unavailable”; “Yes”; “Request for more information”; “Yes, with questions”; “Check back later for definitive answer” and “I will get back to you”. (2017) These coded responses were then analyzed to test the experimental hypothesis.

Overall, the researchers efficiently incorporated numerous types of computational approaches into the experiment to answer the question, including human computation, web scraping, and other automated processes. With these methods, the researchers were not only able to collect data that were impossible to be captured using traditional experimental approaches, but also able to effectively reduce the cost of the experiment.

####Assessment of effectiveness (Edelman, B., Luca, M., & Svirsky, D. (2017).)

The researchers did a solid job to ensure the effectiveness of the experiment: the theoretical construct was smoothly transformed into statistical tests, automated processes were used to reduce the likelihood of human errors, and the external validity was also significantly improved compared to the previous observational study. However, that does not mean that the experiment is flawless. In fact, there are still some potential concerns and limitations that may undermine the validity of the outcomes.

While the researchers did a decent job minimizing the possibility of human errors by employing automated processes and using relatively straightforward experimental procedures, there are still some potential concerns about the internal validity of the experiment. For example, errors can be introduced during the coding process performed with Face++ and human computation. As a matter of fact, even the most advanced facial recognition program cannot ensure a near perfect accuracy when it comes to judging the race. 

The experiment has greatly improved external validity compared to the previous research. First, instead of selecting the sample entirely from a single city, this experiment included sample Airbnb hosts from five different cities in U.S. Noticeably, while the number of cities is still very limited, these five cities (Baltimore, Dallas, Logs Angeles, St. Louis, and Washington, DC) have varying Airbnb usage and come from diverse geographic regions. Besides selecting samples from diverse cities, the researchers took actions to further ensure the external validity of the experiment by comparing the results with observational patterns from past published studies. For example, the researchers found that the 8-percentage-point penalty for African American guests is consistent with the racial gap found in contexts ranging from labor markets to online lending to classified ads to taxicabs.

Noticeably, the researchers also took advantage of observational data to analyze the heterogeneity of treatment effects. Specifically, by analyzing the effects of host characteristics on the treatment effect, the researchers were not only able to further test the robustness of the experimental outcomes, but also able to find clues about the casual mechanism of observed racial discrimination. According to the results, the racial discrimination was not likely to be driven by homophily (in-group bias). Also, the racial discrimination against Black guests was found to persist across most subgroups of hosts, and was not strongly influenced by factors such as the whether the Airbnb host is a professional/casual, whether the property is shared, or the age of the host. However, the researchers observed that hosts who recently accommodated Black guests were unlikely to discriminate against Black guests.

Despite its relatively strong validity, the experiment may raise concerns on its ethnicity. First, the researchers might have violated the principle of Respect for Persons: although it is understandable that the researchers could not seek for consents of the participants prior to the experiment, they did not seem to inform the hosts even after the study. Moreover, by contacting a large number of hosts with fake accounts, the researchers potentially violated the policies of Airbnb. In fact, if the hosts found out that the accounts were fake, there would a chance of compromising the credibility and image of the Airbnb platform.

Overall, the experiment effectively proved the existence of racial discrimination against Black guests on the Airbnb market. More importantly, such racial discrimination was found to persist across numerous groups of hosts and geographically diverse cities.

####The value-added of conducting both research projects

While the observational study and the experiment tackled the racial discrimination on Airbnb from two different perspectives, conducting these two researches together can have considerable synergetic effects.

First, while the researchers did not look deeply into the casual mechanism of observed racial discrimination in the observational study, the findings in the experiment may help rule out some possibilities and thus revealing the true underlying mechanism. For example, in the experiment, the researchers found that racial discrimination against Black guests was not driven by in-group bias and was not related to most host/listing characteristics. Moreover, based on the finding in the field experiment that discrimination disappears among hosts who recently hosted Black guests, we may hypothesize that guests who recently rented apartments of Black hosts also tend not to discriminate against Black hosts.

Moreover, while the lack of validity in the first study severely compromised the generalizability of the research results, the fact that the racial discrimination was found relatively consistent across five diverse cities may make it slightly more reasonable for us to apply the results from the observational study to other U.S. cities. That being said, further studies should still be conducted before we can conclude that the racial discrimination against black hosts is universal, instead of case-specific.

Furthermore, the observational study also helps to eliminate concerns on the experiment. In contrast to the non-reactive nature of observational studies, experiments are always accompanied by concerns on experimenter effect (also known as observer-expectancy effect, or observer effect), which states that participants tend to behave differently in an experimental setting. While the researchers did a solid job hiding their intentions in the experiment, we can still not complete rule out the possibility that the observed racial discrimination was caused by reactivity. However, the fact that a somewhat similar pattern of racial discrimination was also observed in the non-reactive observational study definitely helps assure that the observed racial discrimination was not due to reactivity.

Last but not least, proving that racial discrimination exists both on the supply side and onthe demand side not only helps to raise concerns and awareness on this issue, but also provides some grounds for future establishment of policies or regulations on racial discrimination in the online market.

####Digital survey-based research

There are numerous approaches to test the existence of racial discrimination (against Black people) on the Airbnb market, or similar online platforms. For example, a digital survey-based research can be conducted as an improvement to the observational study conducted by Benjamin Edelman and Michael Luca. 

The proposed survey-based research strives to answer the same question the researcher raised: does racial discrimination against Black hosts exists on Airbnb? However, the proposed survey-based research is expected to have a better external validity and generalizability. Conducting a survey-based research instead of directly using observational data also helps us to avoid omitted-variable bias through having better control on relevant variables.

The survey will be distributed online, and will contain information of a host as well as an apartment listing. The photo of the host is omitted for better control of the experiment and isolation of casual mechanism, and all fields will be held constant except for race, gender, and name. Then, using a similar of assigning treatment groups in the field experiment, we generate four groups of simulated hosts, namely Black male, Black female, White male, and White female. Then, the host information is assigned to each survey using a random process. As for the apartment information, we can use 4 different real listings from 4 geographically diverse cities. The listings are also randomly assigned to each survey, and all the specifics of the apartments available on Airbnb will also be provided.

Then, each participant will be asked to answer the following core question: if you were a visitor to this city, how much would you like to pay to rent this Airbnb apartment for one night?  (Choose from score 1 to 11)  
1:  0.5 P  
2:  0.6 P  
...  
6: 1 P  
…  
10: 1.4 P  
11: 1.5 P  

(*Note: P is the listed rental price of the apartment on Airbnb, and will very for each different apartment*)

Besides answering the question, participants are also required to provide some basic background information, including gender, race, and age. Moreover, participants will also need to identify whether they are Airbnb users.
After the responses are collected, we can then test our hypothesis by comparing the average scores (prices) reported for each group. For example, if racial discrimination against black hosts does exists, we expect to see a lower average score reported for the surveys with Black hosts compared to those with White hosts. Moreover, similar to the field experiment, we should be able to test the robustness and the casual mechanism of racial discrimination by comparing average scores cross different groups.

This survey-based approach offers some advantages over the observational study. First of all, by comparing prices offered for the same apartment instead of different apartments with similar "perceived quality", and by omitting the profile photos of hosts, we can eliminate the potential omitted-variable bias. Moreoever, by using several apartments from different cities, we can not only investigate whether racial discrimination is city-specific or a more universal phenomenom, but also better increase the external validity of the experiment and derive more generalizable results. 

However, there are also some obvious drawbacks to this survey approach. First, observer bias not only exists in experiments, but also in survey researches: if survey respondents know that we are conducting a survey research on racial discrimination, they might alter their behaviors accordingly even in an anonymous survey. Therefore, we need to carefully hide our intentions by telling the participants that we are conducting a research on rental pricing of apartments. Another drawback to the survey study is associated with the increasingly low response rate of survey researcher. To increase the survey response, we can offer monetary incentives in the forms of a lottery, or a coupon. Last but not least, the survey respondents may not be representative of Airbnb users. Although I do not expect a significant gap in racial discrimination exists between the survey respondents and the Airbnb users, the survey will ask each participant to report if he/she is an Airbnb user, just in case being an Airbnb user will actually affect the possibility of racial discrimination. (Which will also be an interesting finding)

####References cited:

1. **Edelman, Benjamin G., and Michael Luca.** 2014. "Digital Discrimination: The Case of Airbnb.com." *Harvard Business School Working Paper 14-054*
2. **Edelman, Benjamin et al.** "Racial Discrimination in the sharing Economy" *American Economic Journal: Applied Economics 2017, 9(2): 1-22.*
