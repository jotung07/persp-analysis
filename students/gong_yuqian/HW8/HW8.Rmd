---
title: "unsupervised learning"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```
```{r}

```


```{r}
#install.packages("devtools")
library("devtools")
install_github("vqv/ggbiplot", force = TRUE)
library(ggbiplot)
```
Part I problem 1
From the loadings tables, we can see that in the first component analysis, the variables Number of applications accepted, Number of applications recevied, Number of fulltime undergraduates are highly correlated and their loading values are 0.348, 0.557,0.671 respectively. 

For the second component analysis, the loadings for variables decrease a lot. No variable has a loading greater than 0.3. Variables with positive laodings include
Estimated personal spending, Number of applications received, Number of new students
enrolled, Number of applications accepted, Number of partime undergraduates, Number of fulltime undergraduates. They are correlated with each other and contribute to the second component analysis. 
```{r}
College = read.csv("College.csv", header = TRUE)
college1 = subset(College, select = -c(1))
output = prcomp(college1, scale = FALSE)
ggbiplot(output)+
  scale_color_discrete(name = '') +
  theme(legend.direction = 'horizontal')
sort(round(output$rotation[, 1], digits = 3))
sort(round(output$rotation[, 2], digits = 3))
```
Part I problem 2
From the cumulative proportion of variance explained table, we can see that about 0.871
of all the variance can be explained by the first two principal components.
```{r}
output.var = output$sdev ^ 2
pve = output.var/sum(output.var)
cum_pve = cumsum(pve)
cum_pve
plot(cum_pve, xlab="Principal Component ", ylab=" Cumulative Proportion of Variance Explained ", ylim=c(0,1), main = "Cumulative PVE")
```
Part II
```{r}
USArrests = read.csv("USArrests.csv", header = TRUE)
usa = prcomp(USArrests[-1])
ggbiplot(usa)+
  scale_color_discrete(name = '') +
  theme(legend.direction = 'horizontal')
```

Problem 2
The groups are divided based on the observations' scores of the first component analysis. Observations whose scores are negative are in one group and observations whose scores are positive are in the other group. Assault arrests levels are the major contributing variable to the first component analysis.
```{r}
#install.packages('ggfortify')
library(ggfortify)
set.seed(4)
km.two = kmeans(USArrests[, -1], 2,nstart = 20)
km.two
rownames(USArrests) = USArrests[, 1]
autoplot(km.two, data = USArrests[, -1],
         label = TRUE, label.size = 3, loadings = TRUE,
         loadings.label = TRUE, loadings.label.size = 3,
         loadings.colour = "blue")+labs(title = "Clustering with k = 2")
```
Problem 3:
From the graph we can see that the fourt clustering groups are decided based on the score values of each state in the first principal component analysis. Since Assault is the major contributing variable in the first component analysis, we can assume that the clustering groups have different levels of assault arrests.
```{r}
set.seed(5)
km.four = kmeans(USArrests[, -1], 4,nstart = 20)
km.four
rownames(USArrests) = USArrests[, 1]
autoplot(km.four, data = USArrests[, -1],
         label = TRUE, label.size = 3, loadings = TRUE,
         loadings.label = TRUE, loadings.label.size = 3,
         loadings.colour = "blue")+labs(title = "Clustering with k = 4")
```
Problem 4:
From the graph below, we can see that the groups are still divided based on their scores of the first component analysis, which is mainly dominated by their assualt arrests situation. Interestingly, we find what it differs from the graph with 4 clustering groups is that the groups with the lowest scores are merged together to be a single group in this case. The two groups with the highest scores when k is equal to 4 haven't change in this case when k is equal to 3.
```{r}
set.seed(7)
km.three = kmeans(USArrests[, -1], 3 ,nstart = 20)
km.three
rownames(USArrests) = USArrests[, 1]
autoplot(km.three, data = USArrests[, -1],
         label = TRUE, label.size = 3, loadings = TRUE,
         loadings.label = TRUE, loadings.label.size = 3,
         loadings.colour = "blue")+labs(title = "Clustering with k = 3")

```

Problem 5:
Interestingly, the clustering results are the same as the results when we use the raw data. This could be a great evidence that the first and second component analysis scores can explain the majority of the variances of the raw data. 
```{r}
set.seed(7)
prin_score = prcomp(USArrests[, -1])$x
cluster_prin_score = kmeans(prin_score,3)
df_prin_score = as.data.frame(prin_score)
rownames(df_prin_score) = USArrests[, 1]
kmscore.three = kmeans(df_prin_score, 3 ,nstart = 20)
kmscore.three
#rownames(USArrests) = USArrests[, 1]
autoplot(kmscore.three, data = df_prin_score,
         label = TRUE, label.size = 3, loadings = TRUE,
         loadings.label = TRUE, loadings.label.size = 3,
         loadings.colour = "blue")+labs(title = "Clustering with k = 3 on first 
         two principle scores")
```

```{r}
#install.packages("ggdendro")
library(ggdendro)
#library(dendextend)
```

Problem 6
```{r}
hc.complete = hclust(dist(USArrests[, -1]), method = "complete")
ggdendrogram(hc.complete) + labs(title = "Hierarchical clustering with complete
                                  and Euclidean distance")
```

Problem 7:
Clustering group 1 includes states:
 "Alabama"        "Alaska"         "Arizona"        "California"     "Delaware"      
 "Florida"        "Illinois"       "Louisiana"      "Maryland"       "Michigan"      
 "Mississippi"    "Nevada"         "New Mexico"     "New York"       "North Carolina"
 "South Carolina"
 
 Clustering group 2 includes states:
 "Arkansas"      "Colorado"      "Georgia"       "Massachusetts" "Missouri"     
 "New Jersey"    "Oklahoma"      "Oregon"        "Rhode Island"  "Tennessee"    
 "Texas"         "Virginia"      "Washington"    "Wyoming"  
 
 Clustering group 3 includes states:
 "Connecticut"   "Hawaii"        "Idaho"         "Indiana"       "Iowa"         
 "Kansas"        "Kentucky"      "Maine"         "Minnesota"     "Montana"      
 "Nebraska"      "New Hampshire" "North Dakota"  "Ohio"          "Pennsylvania" 
 "South Dakota"  "Utah"          "Vermont"       "West Virginia" "Wisconsin"    
```{r}
hc.2 = hclust(dist(USArrests[, -1]), "complete")
cuttree = cutree(hc.2, k = 3)
plot(hc.2, main = '', cex = 0.6)+ title(main = list("Hierarchical clustering without scaling", cex = 1.5, font = 3))
rect.hclust(hc.2, k = 3)
table(cuttree)
rownames(USArrests[, -1])[cuttree == 1]
rownames(USArrests[, -1])[cuttree == 2]
rownames(USArrests[, -1])[cuttree == 3]
```
Problem 8:
After scaling the data, the observations in each clustering group are different from the case without scaling. We can see that group 3 have more observations now. Before the scaling, the variables with larger variance will be the major contributing factors in deciding which group an observation belong to. However, after the scaling, variables will mostly have equal importance in dividing the clustering groups and I think it is better if we want to compare observations by their overall differences instead of differences in a subset of variables.
```{r}
hc.2 = hclust(dist(scale(USArrests[, -1])), "complete")
cuttree = cutree(hc.2, k = 3)
plot(hc.2, main = '', cex = 0.6)+ title(main = list("Hierarchical clustering without scaling", cex = 1.5, font = 3))
rect.hclust(hc.2, k = 3)
table(cuttree)
rownames(USArrests[, -1])[cuttree == 1]
rownames(USArrests[, -1])[cuttree == 2]
rownames(USArrests[, -1])[cuttree == 3]
```


```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
