---
title: "Homework 08"
author: "Kristopher Nichols"
date: "December 3, 2017"
output: github_document
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

Libraries
```{r setup}
library(knitr)
library(ggplot2)
library(tidyverse)
library(stats)
library(forcats)
library(tidyverse)
library(readr)
library(FactoMineR) 
library(kableExtra)
```

Load Data
```{r Load Data}
College <- read_csv("~/GitHub/persp-analysis/students/nichols_kristopher/Data for HW8/College.csv")
USArrests <- read_csv("~/GitHub/persp-analysis/students/nichols_kristopher/Data for HW8/USArrests.csv")
```

# Cluster College Data
##1 Perform PCA analysis on the college dataset and plot the first two principal components. 

```{r PCA College 1}
College1 <-College %>% 
  select(-Private)
  
pca_college <-PCA(College1, scale.unit=TRUE, graph=TRUE )
pc <-prcomp(College1[,-1], scale=TRUE)
head(pc$x[,1:2])
```

Component one is most strongly associated with PhD, Terminal, Top10Perc and Top25perc, while component two is most strongly associated with F. Undergrad, Accept, Enroll, and Books. The variables associated with component one are a bit scattered, but component two seems strongly associated with variables that have to do with incoming freshman, or application-realted variables.

## 2 Calculate the cumulative proportion of variance explained by all the principal components.

```{r PCA College 2}
kable(summary.PCA(pca_college))

kable(pca_college$eig) # Single out eigenvalues for easier comparison
```

The first two components account for 58.36% of the varaince explained by all components.

# Cluster States
## 1 Perform PCA on the dataset and plot the observations on the first and second principal components.

```{r PCA Arrests 1}
USArrests1 <- USArrests %>% 
  select(-State)

PCArrests <-prcomp(USArrests1, scale=TRUE)
summary(PCArrests)
row.names(USArrests1) <- USArrests$State

pca <- PCA(USArrests1, scale.unit=TRUE, graph = TRUE )
```

## 2 Perform $K$-means clustering with $K=2$. Plot the observations on the first and second principal components and color-code each state based on their cluster membership.

```{r PCA Arrests 2}
arrestHC2 <-HCPC(pca, nb.clust = 2, graph = FALSE)

plot.HCPC(arrestHC2, choice = "map", title = "K- Means Clustering: K = 2")
```

Component one, as seen in the graphs for question one, is associated with general crime: rape, assault, and murder. Thus it is not coincidence that cluster 2 associates higher with this component. As seen later in the assignment states in this cluster report some of the higher crime rates namely: Alaska, Texas, and Florida. Cluster one associates more strongly with component two, which is more associated with UrbanPop. This variable just seems to be a red herring, this cluster more than likely just represents lower crime rate states.

## 3 Perform $K$-means clustering with $K=4$. Plot the observations on the first and second principal components and color-code each state based on their cluster membership.

```{r PCA Arrests 3}
arrestHC4 <-HCPC(pca, nb.clust = 4, graph = FALSE)

plot.HCPC(arrestHC4, choice = "map", title = "K- Means Clustering: K = 4")
```

This clustering adds more specifity to the previous graph. As we can now see that cluster one represents the states with the lowest crime rates, while clusters three and four represent states with higher crime rates, with cluster four representing states such as Florida and Alaska with the highest rates. However this clustering also allows for easier representation of the population component. As incorrectly hypothesized in the previous question, this component may help differetiate high crime rate states with high populations such as Florida, California, and Texas, from states like Alaska (also in cluster four) with high crime and lower population. This is evident in Alaska being lower on the y-axis.

## 4 Perform $K$-means clustering with $K=3$. Plot the observations on the first and second principal components and color-code each state based on their cluster membership.

```{r PCA Arrests 4}
arrestHC3 <-HCPC(pca, nb.clust = 3, graph = FALSE)

plot.HCPC(arrestHC3, choice = "map", title = "K- Means Clustering: K = 3")

km.out <- kmeans(USArrests1, 3)
table(km.out$cluster)
```

This clustering technique appears to differentiate between those states with very low crime and those states with medium crime. However, states with high crime are all clustered together (cluster three).

## 5 Perform $K$-means clustering with $K=3$ on the first two principal components score vectors, rather than the raw data.

```{r Arrests 5}
data("USArrests")
pc <- prcomp(USArrests)
k3v <- kmeans(pc$x[,1:2], 3, nstart = 15)

# Graph made for this question using GGplot due to difficulties with graphin vector values. Labeled axes with Dim1 and Dim2 to allay confusion in comparison. Also included tables to assist with comparing slightly different graphs.

data_principle_comp <- as.data.frame(pc$x[,1:2]) 
data_principle_comp$cluster = as.factor(k3v$cluster)
ggplot(data_principle_comp, aes(data_principle_comp$PC1, data_principle_comp$PC2, color=cluster)) +
  geom_text(aes(label=rownames(data_principle_comp)), size=3) +
  labs(colour = "Cluster", 
       title = "Clustering Diagram: Principle Components One and Two with Score Vectors", 
       subtitle = "Using K-Means", 
       x = "Principle Component 1 (Dim1)",
       y = "Principle COmponent 2 (Dim2)") +
  theme_bw()

prarrest <- prcomp(USArrests1)
kmarrest <- kmeans(prarrest$x[, 1:2], 3)
table(kmarrest$cluster)

# Graph made for this question using GGplot due to difficulties with graphin vector values. Labeled axes with Dim1 and Dim2 to allay confusion in comparison. Also included tables to assist with comparing slightly different graphs.
```

As seen less effectively in the graph, but more effectively in the tables, the clustering of the states is very similar in shape and trend - however there are states that are placed in new clusters. Due to the differing categories, and the many states that overlap this method of clustering seems less ideal. 

## 6 Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.

```{r}
HierarchArrest1 <- hclust(dist(USArrests1), method = "complete")
plot(HierarchArrest1, main = "Hierarchical Cluster Plot", 
     sub = "With Complete Linkage", 
     xlab = "States")
```

In this cluster plot it can be seen that most states with higher crime rates are centered to the left.

## 7 Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?

```{r}
HierarchArrest2 <- hclust(dist(USArrests), method = "complete")
cutree(HierarchArrest2, 3)
table(cutree(HierarchArrest2,3))
```


## 8 Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation $1$. What effect does scaling the variables have on the hierarchical clustering obtained?

```{r}
HierarchArrest3 <- scale(USArrests1)
HierarchArrestSD <- hclust(dist(HierarchArrest3), method = "complete")
plot(HierarchArrestSD, 
     main = "Hierarchical Cluster Plot", 
     sub = "With Complete Linkage and Euclidean Distance", 
     xlab = "States")
```

Scaling the variables allows for better viszualization of the different clusters of both crime rate and population. For example, amongst the righ-most cluster Alaska stands out prominently. When referenced with our four cluster graph these states are the states with very high crime rates and lower populations - Alaska was categorized as acluster four state in that graph despite having such a low population due its apparently high crime rate and this is reflected here. However, a state like Florida, which may have stood out as the most prominent state in terms of crime using alternative clustering techniques, here it is given the context of higher population states within its hierarchical cluster (e.g., New York, Texas, Illinois), and its prominence is a bit diminished, and perhaps more accurately shown. 