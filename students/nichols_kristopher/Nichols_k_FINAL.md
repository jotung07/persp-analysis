Final
================
Kristopher Nichols
December 6, 2017

**Summarize the research design and explain how the research design leverages computational methods to ask and answer a question.**

1
=

The 2014 study conducted by Edelman and Luca features an observational design with a matching component. The purpose of the study is to investigate the effects that having a profile picture can have on discrimination against black hosts, and the effects that this discrimination may have on prices of host. The design employed by these researchers utilizes computational methods to obtain the profile pictures of all New York City Airbnb landlords, and combines this data with other information that characterizes their profile such as prices and quality of their Airbnb property. Once the data is combined, researchers utilized Amazon Mechanical Turk to code the hosts as either White, Black, Hispanic, Asian, Unclear but Non-White, Multiple Races, NA, or unclear. Amazon MTurk workers were also utilized to code the quality of the apartment, allowing for controlling of this quality variable. This study leverages computational advantages through its downloaded pictures combined with Airbnb data, to create one profile that can be used to map various variables across races. Furthermore, this study is uniquely benefited by utilizing computational methodology due its reliance on MTurk workers (a mass collaboration component) to accurately code race and apartment quality, and later control for this apartment quality in its comparisons across race (again only achieved due to the computational combining of picture with Airbnb apartment qualities) - allowing for a more salient analysis. To control for apartment quality in an observational design, it seems that researchers employed a matching design - that is, they utilized apartment quality as the variable that they matched across, and identified black and non-black groups.

2
=

The 2017 study conducted by Edleman, Luca, and Svirsky utilizes a digital- field experiment to investigate a similar research question as their 2014 study - how prevalent is discrimination on Airbnb. They utilized computational methods through their scraping of the Airbnb website to gather information about all properties listed in Dallas, Baltimore, Los Angeles, St. Louis, and Washington. With this information, they then utilized scraping tools they constructed to gather information from each of the hosts profile, including their profile picture. Then, utilizing Amazon Mechanical Turk (again, a mass collaboration computational component) they coded the host's gender, race, and age. Two workers were used, with a third entering upon disagreement. Researchers then utilized Face++ face detection API (application programming interface) to categorize pasts guests who had reviewed each host - former guests were also categorized by race, gender, and age. Researchers hoped to utilize this data to compare host's history of response towards African-American request with new African-American requests. Finally, information on listing was also scraped for both basic information (price, number of bedrooms, etc.), and yielded important information such as longitude and latitude - allowing for linkage to census data regarding demographics in the area.

Researchers utilized four treatment groups, across which they spread 20 researcher created Airbnb accounts that differed in no way other than their name (no picture was included). The treatment groups were African-American male signaling names, African-American female signaling names, white males and white females - this data was based on Massachusetts baby name between 1974 and 1979 and was referenced with a scale in which they validated the names. Finally, each host was randomly assigned one of the 20 Airbnb accounts and a message was sent out requesting stay, if the host replied, a simple message was sent back indicating ambiguity regarding plans as to not harm the host's finances.

This study appears to have been much more computationally rigorous than the 2014 study. In order to investigate their question, researchers first developed scraping tools to gather all of their data, including: pictures and profile data. Secondly, this study involved the creation of 20 guest Airbnb accounts that were likely programmed to respond automatically to the hosts response messages. This study also involved the use of MTurk as a means for coding hosts, and Face ++ for coding past quests. Each of these steps were pivotal for comparison purposes; the multiple cities in which they scraped data was important for enhanced external validity and likely not possible without computation methods. The scraping allowed for a relatively large sample size of Airbnb hosts, and allowed for collection of data without the need for Airbnb approval. Finally, utilizing past guests in comparison with the new African-American guests allowed for greater internal validity - confirming the effects were still significant across each host.

**Evaluate the effectiveness of each paper's research design independently. That is to say, what do we learn from each paper on its own? What are the limitations of each paper on its own?**

1
=

The 2014 study achieves its conclusion by leveraging the advantages of big data in a number of ways. The study is highly non-reactive, due to it simply being an observational design, the participants were completely unaware of the nature of the experiment, and were thus unable to experience reactance. For example, in the 2017 study Airbnb noticed the researchers experiment and moved to block their fake accounts' access, potentially biasing the results, and severely limiting the population size to only five major cities (out of an expected 20). Here, there is no such problem. Also, this study leverages the always-on benefits of big data much to its advantage. As opposed to being limited to only collecting hosts during a certain time, or of a certain availability, the researchers were able to collect data on hosts through the entire day of July 17th, and could have obtained much more data had they desired - they were not limited in their sampling, contributing to the high external validity of this study. Finally, this study featured a large sample size, but more importantly featured a great amount of data on each individual host. Because of the nature of the computational methods the researchers employed, many more variables were able to be documented on each host, allowing for better controlling and better internal validity in their conclusions - this is perhaps the best advantage of this study. Due to its simple observational design and comprehensive data that was wrought, the researchers were able to conduct a number of statistical analysis, including those that controlled for possible confounds, allowing for more impactful and internally valid results. One of its advantages that increase internal validity includes the use of Mechanical Turk workers to serve as potentially more accurate coders than a machine. These advantages attributed to the addition of big data and computational methods allowed researchers to reach more valid and precise conclusions regarding the effects of discrimination on host price.

However, there are also some disadvantages that arise from the inclusion of big data and computational methodology. For example, this dataset is likely incomplete in that it does not capture all the information desired. Firstly, the study is set only in New York limiting its external validity a great deal due to the possible impact of regional differences on demographics. Secondly, the study is only able to address how hosts with pictures are discriminatory and could be missing out on a segment of the Airbnb population. Furthermore, the way that Airbnb is used in New York could differ compared to other states. Another disadvantage of big data in this study is its potential inaccessibility. Due to this study and the lack of permission on behalf of Airbnb, the likelihood of them allowing for public data (if it is public, if it is not, this is obviously a huge accessibility issue in and of itself) is very low. Ultimately these elements detract from the study's external validity. While benefits make it a study high in internal validity, the incompleteness of its sample ultimately limits its generalizability. \# 2

The principle benefits that the 2017 study receives from its use of computational methodology and big data include its bigness, its completeness, and its high external validity. Concerning its bigness, this study improves upon external validity issues inherent in the 2014 study by including five very large cities. Furthermore, the use of Mechanical Turk workers as coders to not only accurately code race, gender, and age also ameliorates the study's internal validity, as does its use of surveys to insure the accuracy of their name groups. Another big benefit afforded by the large scale of the study is its relative completeness. This study includes a far greater sample, and is thus more externally valid. Furthermore, the field experiment design allows for even greater external validity - allowing for more accurate representation of what an African American may really experience in attempting to find lodging through Airbnb. The greatest strengths of this study include its large scale and the external validity resulting from a larger sample size and its field experiment design. However, there are also issues in this study caused by its computational methodology - and mostly by the researcher's decision to not hide the activity of their study. Firstly, this study is likely to fall prey to system drift. Due to the researcher's choice to not hide their experimental activity from Airbnb, the company was able to discover and combat their sampling techniques, possibly impacting their sampling techniques the longer they sampled and affecting the ways in which the system was accessed. Furthermore, this problem affected the non-reactivity of the study as well - potentially biasing their sample. Yet another way in which this decision impacted their study was the inaccessibility that this study likely resulted in - it is likely more difficult to access this data now than it was previous to Airbnb discovering the researcher's methods. An additional question that should be addressed on behalf of both studies is the potential sensitivity of the data. Due to the nature of the study, participant's personal information was made available to researchers- information that could be potentially dangerous in the wrong hands, further emphasizing another possible disadvantage to this line of research.

**Identify the value-added of conducting both research projects. That is, what do we learn from running both an observational study and a field experiment that we could not learn from just one of these methods?**

The principle advantage of conducting both a field experiment and an observational study is that researchers are able to capitalize on the strengths inherent in each design - which in this case cover the weaknesses of the other study. For example, the principle strength of the first study (the 2014 study) was its strong internal validity, as a part of its observational design and its many controlled variables, it also may lead to potential forecasting of how the trends of discrimination may continue if unchecked. However, this study suffered from a lack of external validity due to its limited sampling techniques. The study also exhibits a lack of explanatory power due to its observational design, which limits its ability to make conclusions about the way in which discrimination may be functioning in all Airbnbs, and not just the Airbnbs of New York. These are issues that the field experiment (the 2017 study) capitalizes upon. With the researcher's knowledge that this effect exists in some context, they were then able to investigate the contexts in which this effect is salient - by both increasing its sample size dramatically and utilizing a design which may allow for grater explanatory power - actually having hosts react to the names of African American individuals. Also, solely in terms of content, the studies accent each other very well, the 2014 focusing on the effects of discrimination towards Airbnb hosts (and consequently the effect of profile pictures), and the 2017 study focusing on the discriminatory effects that individuals with more distinctly African American names experience. Therefore, together the studies play on the weakness of the other looking to sure up the concept the researchers are looking to prove by improving upon both its internal and external validity.

**Consider how you could apply a digital survey-based research design to the primary question of interest from these two papers. What are the potential drawbacks to a survey approach? How might you overcome these drawbacks?**

I propose an ecological momentary assessment (EMA) design for the question of discrimination towards African Americans in the Airbnb industry. I propose developing an app or website in which we would ask a random sample of the most frequent African American Airbnb users to utilize. We would ask these participants to take a number of surveys documenting the number of times they applied for an Airbnb and were approved or denied and the speculated race and age of the host who denied them. After their Airbnb session was terminated, participants would then be prompted to answer survey questions regarding their stay and inquire about whether they feel they had been discriminated against and in what way. I believe that this would achieve fantastic internal validity and investigate an aspect of discrimination towards African American's not broached in the other studies: Once African American's are able to procure Airbnb placement, how do they feel that they are treated? This data could even be used longitudinally to track how African American's feel that they are treated with fluctuations should any fluctuations in their guest grade arise. This design would also avoid reliance on a sample with a picture of themselves, or a more distinct African American name - and would allow for the opinion for all Airbnb users - allowing for potentially fantastic external validity as well. However, there are immediate issues including the technological capabilities needed for this experiment, the possibly ethical quandaries on behalf of the host not being informed that they are involved in an experiment, and problems with survey questions. Technological obstacles on behalf of the research time are unavoidable but could be achieved with enough grant money or through a large enough school. Technological concerns on behalf of the participant (i.e., potential limited knowledge of technology affecting their ability to answer survey questions) could be resolved using learning sessions before the beginning of the experiment. The problem of survey questions and their construct validity is troubling, but measures could be taken to use valid survey methods which are well vetted within discrimination literature to achieve the greatest internal validity possible. And finally, the ethical concern of the hosts could be dealt with by simply informing them of their involvement within the study - although this may bias the sample, this is an ethical concern that should be weighed in terms of protecting the participant through the respect for persons principle of Salganik.
