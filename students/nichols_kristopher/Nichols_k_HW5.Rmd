---
title: "Kaggle Open Call"
author: "Kristopher Nichols"
date: "November 12, 2017"
output:
markdown_document:
  theme: readable
---
#Question 1: Part II

I found the WSDM - KKBox's Music Recommendation Challenge to be very interesting. It is a competition started by the 11th ACM International Conference on Web Search and Data Mining and utilizes data provided by Asia's leading pop station. Using this provided data it tasks the competitors to create a better music reccomendation system than is currently available with computational music prediction techniques. A key problem with music prediction algorithms as currently constituted is that they often lack historical data to make accurate predictions with, and this is a problem the competition specifically targets. As stated, the central task is for competitors to predict the likelihood of a listener listening to a song "repetitively after the first observable listening event within a time window was triggered." The team with the highest predictive score is awarded $2,500 which will be presented at the conference.

Submission for the competition is relatively simple and stripped down. The competition is open to all over the age of 18 and although public sharing is allowed, private sharing of data is not. All that is needed to submit is to register online via kaggle. Competitors are given training data and are encouraged to use outside data to increase their predictive power. 

#Part III
```{r global_options}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```
## Load Libraries
```{r}
library(readr)
library(ggplot2)
library(dplyr)
```

##Load Data
```{r}
video_game_sales_with_ratings <-read_csv("~/GitHub/persp-analysis/students/nichols_kristopher/Data for HW5/Video_Games_Sales_as_at_22_Dec_2016.csv")
View(video_game_sales_with_ratings)
```

##Clean Data
```{r}
video_game_sales_with_ratings <-subset(video_game_sales_with_ratings, !is.na(User_Score))
video_game_sales_with_ratings1 <-subset(video_game_sales_with_ratings, User_Score != "tbd")
```

##Make Graph
```{r}
video_game_sales_with_ratings1 %>%
  ggplot(aes(Critic_Score, User_Score, color = Critic_Score)) +
  geom_point() +
  theme_bw() +
  scale_y_discrete(breaks = seq(0, 10, by = 1)) +
  labs(title = "How User Score Relates to Critic Score", x = "Critic Score", y = "User Score", subtitle = "MetaCritic Data", color = "Score")
```

#Question 2: Improving a Journal Article

In the paper "Co-benefits of addressing climate change can motivate action around the world" by Bain et al. [2016](https://repository.up.ac.za/bitstream/handle/2263/51902/Bain_CoBenefits_2016.pdf?sequence=1) researchers looked to uncover which framing of climate change benefits most effectively convinces individuals to enact in pro-environmental behavior. Using a global sample, researchers found that emphasizing on the benevolent (moral caring) and developmental (societally beneficial) co-benefits of climate change, that is, the potential for all to prosper resulted in greater likelihood of action. 

This study is very interesting, but has one major pitfall: although the sample is global, the sample is still not very high in external validity because it solely utilizes university students. This is an issue that could be controlled for very well with a human computation element. For example, I propose that the research team construct websites for each country that is planned to be a part of the study. On this website the researchers will train participants in articulating their feelings in an analyzable way, identifying their political affiliation, and will be prompted to enter in their feelings on climate change. 

Instructions will include that participants should emphasize the most important element about climate change for them, and why policy tackling the issue is good *and* bad. Each website for each country can be peddled by bots on twitter or Facebook, and the website can be trained to search for key phrases such as "it helps everyone" or "too expensive." This data can then be combined, and mapped cumulatively but also piecewise with many different analyses including random forest analysis.

#Question 3: Alternative Assignment

Three new methods for tracking influenza that have arose with increasing computational power include Google Flu Trends, InfluenzaNet, and traditional influenza tracking systems featuring more human involvement with a network of physicians involved to track ILI (influenza-like-illness. Firstly, these traditional methods as featured in this [study](http://www.bmj.com/content/339/bmj.b3403) by Elliot et al. (2009) involves a system run by trained nurses and other health practitioner which assign diagnoses and notes primary symptoms. This data, when combined across many practitioner, ideally, becomes representative of the status of influenza in a certain area. This strategy is advantageous because it allows for quicker (as compared to diagnosis based on data by those who go into see a doctor) diagnoses of the status of influenza in an area and involves trained professionals who are more likely to give a meaningful diagnosis. However, this strategy is also far less quick than more computational models can be, and also involves a great deal of deliberate action - i.e., someone must first realize that they are sick and call one of these hotlines for the data to be recorded. 

This is an aspect that strategies like Google Flu Trends and InfluenzaNet look to control for. Google Flu Trends looks to make assessments of influenza status through the use of search data and historical trends of influenza. This allows for an always-on approach that traditional methods simply cannot match due to the huge population Google Trends can draw from. While those that may voluntarily call one of these hotlines may be demographically biased in some way, while Google is nearly ubiquitous.  Furthermore, this strategy may lead to more meaningful and quick assessments of symptomology as simply queries such as fever and runny nose may alert these tracking systems and record potentially meaningful data. However, although access may be a net positive the drawback of internet access is nonetheless meaningful and this sort of computation-only strategy is hugely dependent on advanced algorithms and majority computer access in a country. Also, this strategy is more open to confounds, such as an epidemic may engender public worry and encourage more people to search more frequently for symptoms, even if none exist. This could bias the data in potentially catastrophic ways and would involve a great deal of controlling to fix the algorithms.

Forming the middle ground between these two approaches is InfluenzaNet. This strategy recruits those who are sick to take online questionnaires that will be used to track influenza activity. This strategy features many of the benefits of Google Trend, it is quicker than traditional methods, is always-on and is more likely to capture representative samples of the population, and demands far less from human workers. However, it also involves some of the drawbacks from traditional methods as it is not entirely free from human hands - participants must still be recruited and they must also exert extra effort and take the questionnaire. 

The effect of these pros and cons are very visible in an example such as a sudden epidemic of the swine flu. While traditional methods may be much slower they are the most likely to do a consistently accurate job of diagnosing. However, in the example of an epidemic the drawback of slowness becomes even more important. While Google Trends may be much faster and always-on, capturing more accurate pictures of the outbreak, the panic of the outbreak may lead to inaccurate models of the status of the flu. What may be most advantageous in the example of a sudden outbreak may be the middle ground (until Google Trends advances more) of InfluenzaNet. This would provide greater speed for diagnosis in addition to less potential biases from an overflow of panic ridden data.
	

