# Evaluating research designs 

Shuting Chen 

12/06/2017

## Part 1: Description and evaluation of the research design for each paper 

### Paper 1: Digital discrimination: the case of Airbnb.com (2014)

#### Description of the research design 

In this research project, the authors explore the online race discrimination by estimating the price gap in rents received by non-black and black hosts on Airbnb. To examine the extent of discrimination, they employ a new data set which combines listing pictures of all New York City hosts on Airbnb with their rental prices and information on characteristics of their properties. More specifically, the data set contains listing-base information consisting of the posted price, characteristics of the host, characteristics of the property and the average rating for each property characteristic such as location and cleanliness. Since the purpose is to investigate the price discrimination by races, the authors need to identify the race of the hosts on Airbnb. Therefore, they hired workers on Amazon Mechanical Turk to code the race of the hosts into one of eight predetermined categories by providing them with the profile pictures of the hosts. Moreover, the quality of each listing is also rated by workers on Amazon Mechanical Turk after they evaluate the photos of each listing. It is reasonable to use crowdsourcing Internet marketplaces such as Amazon Mechanical Turk to conduct these human computation tasks which can be easily broken into many simple tasks and can be competed without experts. 

To extract the effect of races on the difference in prices, the researchers attempt to control other factors relevant to price determination on Airbnb as much as possible. Specifically, they not only control for the main characteristics of the listing such as the number of bedrooms and whether it is a shared property but also control for the effects of guest perceptions of location, quality, etc. and the quality of listing photos. Before finalizing the conclusion, the authors also conduct the robustness check to examine the effect of classifying hosts into other racial groups rather than black and non-black on the rents. It turns out that no matter comparing black hosts to the non-black or the white, they earn roughly 12-15% less for a similar property with similar rating and photos. 
 
#### Evaluation of the effectiveness of the research design 

As a digital enhanced observational study, this paper investigates how additional information about personal profiles facilitate discrimination by measuring the difference in rents between non-black and black hosts on Airbnb. Although the authors can easily access to a large volume of public data available on the website of Airbnb and construct them for their research purpose, they still suffer from two common drawbacks, which are incompleteness and inaccessibility. Since Airbnb refused to share more data relevant to demand side, the researchers have to forego analysis of consumer demand, which is one of two determinants for the price of products. 

For internal validity of the research design, the underlying challenge is how to correctly isolate the effect of races on the gap of rents without including the effect of confounders. The authors try to mitigate potential related factors by two steps. Firstly, identify determinants of prices on Airbnb as well as their importance on determining the price. Secondly, control these determinants separately and together to figure out the impact of races on price discrimination. Although the researchers control for all visualized information, some unobserved factors between listings may partially account for the racial differences in rents. Therefore, it would be better to visualize some unobserved features by proxy variables so that we can yield more reliable results. Besides, this paper mainly focuses on the rents gap between non-black and black hosts. What if we extend analysis to other racial groups? Is there any notable difference in rents among other races?
  
One issue of the external validity for this study is that it only employs data in New York City. Can the proportion of black hosts in New York be proper representative for hosts on Airbnb in other areas? Could the results potentially overestimate the price gap because of not considering some special determinants related to New York? Therefore, without further analysis or justification of why only selecting data from New York, it is suspicious to believe that the results from this study have the universality. In addition, the data were collected only within one day. Can we obtain the similar results if we collect data over some time periods rather than a single day? Would we gain extra universality by including data covering a relatively long time period? 

### Paper 2: Racial discrimination in the sharing economy: evidence from a field experiment (2017)

#### Description of the research design 

In this paper, the authors conduct a field experiment on Airbnb to examine the racial discrimination on the procedure of accepting prospective guests. Specifically, they try to figure out whether there exists difference in acceptance rate of guests only in terms of their names and they mainly focus on the difference between distinctively African American names and white names.
 
The data were collected on all properties on Airbnb in five cities, which are Baltimore, Dallas, Los Angeles, St. Louis, and Washington, DC with the use of new built scrapers. Although some hosts have multiple listings, the researchers select only one listing per host with the use of a random number generator. Moreover, they collected data about characteristics of each host and extract detailed information of race, gender and age by hiring workers on Amazon Mechanical Turk. Usually each host profile image would be evaluated by two workers; however, once disagreement appears between these two workers, a third worker would be required. Therefore, the data are reliably classified by race, gender, and age. 

The researchers use following four treatment groups based on the combination of gender and two racial groups: African American males, African American females, white males, and white females. There are five test Airbnb guest accounts within each group and they are identical in all respects except for guest names. For instance, guest names in the group of African American males are the most distinctively African American male names. These names are selected by a survey which requires participants to quickly identify each name as white or African American within three seconds. Hence, it is proper to believe that the names used in each group are most representative for that racial group. After determining treatment groups, the authors sent messages from these test accounts to prospective hosts who are randomly assigned to one of 20 guest accounts. The messages were sent by email with content of asking hosts about a weekend roughly eight weeks distant from when the message was sent. 

After sending totally 6,400 messages to hosts, the researchers tracked the responses over 30 days and categorized each response into 11 categories. Rather than considering all categories, they mainly concentrate on the straight response of “Yes”. Finally, the authors yield persistent results in the overall effect without additional controls as well as with controls related to characteristics of the hosts, properties or listings. 

#### Evaluation of the effectiveness of the research design 

The underlying finding is that applications sent from accounts with distinctively African American names are approximately 16% less likely to be accepted compared to their counterparts with distinctively white names. This result is persistent after controlling variables relevant to characteristics of the hosts, properties or listings. 

For evaluating the effectiveness of the research design, we first consider the use of digital data. Although the researchers adopt new built scrapers to collect data and contact hosts by web browser automation tools, which can help them quickly obtain many data at low costs, their research is still somewhat restricted by inaccessibility and incompletion of the data. Since Airbnb rapidly blocked their automated tools they had to stop collecting data after five cities. With data collected from five cities, one may concern about the non-representativeness among the data. The authors have justified that the selected cities having various levels of Airbnb usage and from diverse geographic regions. It seems appropriate to use these five cities but the non-representativeness could probably be mitigated if we have access to data in other regions.

For internal validity, this study correctly implements the random assignment of treatments. However, I think it would be better if the number of test accounts within each group could increase a bit more. Presumably, the reason why the researchers only create 20 test guest accounts is that they do not want to affect the normal operation of short-run rentals on Airbnb. Nevertheless, sending 6,400 messages to different hosts from only 20 accounts over three weeks is a bit weird in the real-world scenario. We are not sure whether this would cause any issue or induce any drift in the data collected for tracking. Another limitation is that this experiment does not include the effect of past reviews on discrimination. If this study actually follows the mechanism of statistical discrimination, the results seem biased as positive reviews from previous hosts might decrease the degree of discrimination.  

For external validity, as mentioned before, one may concern that whether similar results can be reached if we conduct the same field experiment to other regions. Besides, can we end up with similar results by implementing the experiment in a different scenario? For example, rather than comparing the acceptance rate of African Americans to white Americans, we can compare the same object between African Americans and all other Americans. And what will happen if we use computer-generated profile pictures as the signal of guests’ races rather than distinctive names? Although there still exists threats to the external validity, one notable strength of this study is that the authors evaluate the external validity by comparing their experimental results with patterns of observational data. 

For heterogeneity of treatment effects, classifying hosts by race, gender and age makes it possible to assess the heterogeneity within each treatment group. Table 3 in the paper provides the differences between males and females for guests with African American names. However, it is possible to further explore the heterogeneity in acceptance rates via the dimension of age within each gender group. 

In terms of the ability of identifying the causal mechanism, this experiment does not provide an explicit channel. It is hard to recognize the pathway through which the treatments of distinctive names have an impact on acceptance rate. Probably, hosts reject rental request because they think guests with African American names are more likely in lower socioeconomic status rather than the reason that they are African Americans itself. Therefore, we do not know the intrinsic mechanism of why applications of guests with distinctively African American names are more likely rejected by hosts. 

## Part 2: Link two papers together 

### 2.1 Identify the value-added of conducting both research projects 

In the observational study, the researchers investigate the racial discrimination from the hosts’ side while the researchers in the field experiment explore the same underlying question from the guests’ side. In other words, the supply side of rental market (hosts) suffers from the discrimination in the observational study while the demand side of rental market (guests) suffers from the discrimination in the field experiment. 

As mentioned above, one limitation of the observational study is that it foregoes the consumer demand due to the data restriction. This issue could be mitigated or even resolved with the complement of the field experiment, which offers a channel to obtain data from the consumer demand. Moreover, with the data of five cities collected in the field experiment, the observational study could assess the external validity of its results and reach more proper conclusions with additional enhancement in the external validity.  

Besides, the observational study includes data of ratings for each listing’s characteristics. Potentially, we could combine this with reviews from previous guests included in each listing to figure out the proportion of contribution made by African Americans to property ratings. This could be helpful to identify the causal mechanism of the field experiment, given a hypothesis that hosts do not want to rent their apartments to African Americans because they are more likely to give lower ratings. 

### 2.2 Apply a digital survey-based research design 

#### 2.2.1 Research design 

To investigate the racial discrimination on Airbnb with a digital survey-based research, we would like to conduct a pop-up survey on the website (or app) of Airbnb asking both hosts and guests about their opinions and previous experience relevant to racial discrimination. When people browse the website or app, the survey will appear only when the embedded automation tools recognize the viewers as those who have already had at least ten interactions with others in the community of Airbnb. Behaviors like contacting the hosts or guests, booking an apartment or making a transaction would all be counted as an interaction. 
 
In the survey, people would be asked several types of questions. The first type questions are about their demographic information such as gender, age range and their self-identification of race. The second type questions ask for their previous experience related to racial discrimination on Airbnb. If they previously suffered from the racial discrimination, the question would ask them to identify the form of discrimination by providing them with some options as well as letting them enter their own case. The third type of questions aim to reflect their views related to racial discrimination. For example, the question would offer four names signaling people’s race and ask participants to select a person they would like to rent their property to. Participants do not need to complete the survey at one time and they can pause and return to the survey at their convenience. To maintain the participation at some level, every time the participant views the website or app, the system would remind him/her to continue the survey. Although the website or app can identify the participants by their user names, this information will be automatically concealed when recording his/her survey answers. 

After gathering the data, we intend to identify the racial discrimination conditions by gender, age and race.  Additionally, we would like to further separate differences within each group. For instance, we could identify the most common view about racial discrimination for each age group among all females. 
 
#### 2.2.2 Potential drawbacks and how to overcome them 

The most intractable drawback is that we need to reach an agreement with Airbnb to implement a pop-up survey. It is conceivable that this request is very likely to be rejected since this survey cannot create explicit benefits for the company and the company is exposed to the risk of accidentally leaking confidential stuff. However, since most companies have their own surveys about users’ experience we could probably ask them to add extra questions about racial discrimination which would be important to increase users’ satisfaction. After explaining the increasing concern of racial discrimination on Airbnb and how the company can benefit from our research, we might have the chance to obtain the data. 

Another major concern is that even if we can get the permission to conduct the survey, there will not be enough participants who finish the survey. Presumably, we could increase users’ incentive by providing some discounts for those completing the survey within a given period. 



